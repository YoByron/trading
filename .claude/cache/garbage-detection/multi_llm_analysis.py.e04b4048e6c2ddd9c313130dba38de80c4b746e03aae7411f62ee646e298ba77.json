{
  "dead_imports": [],
  "commented_blocks": [],
  "empty_functions": [],
  "unused_variables": [
    {
      "line": 40,
      "variable": "GEMINI_3_PRO"
    },
    {
      "line": 41,
      "variable": "CLAUDE_SONNET_4"
    },
    {
      "line": 42,
      "variable": "GPT4O"
    },
    {
      "line": 43,
      "variable": "GEMINI_2_FLASH"
    }
  ],
  "all_functions": [
    {
      "name": "create_analyzer",
      "line": 1053,
      "code": "def create_analyzer(api_key: Optional[str]=None, use_async: bool=True) -> MultiLLMAnalyzer:\n    \"\"\"\n    Create a MultiLLMAnalyzer instance.\n\n    Args:\n        api_key: OpenRouter API key\n        use_async: Whether to use async mode\n\n    Returns:\n        Configured MultiLLMAnalyzer instance\n    \"\"\"\n    return MultiLLMAnalyzer(api_key=api_key, use_async=use_async)"
    },
    {
      "name": "__init__",
      "line": 125,
      "code": "def __init__(self, api_key: Optional[str]=None, models: Optional[List[LLMModel]]=None, max_retries: int=3, timeout: int=60, rate_limit_delay: float=0.5, use_async: bool=True):\n    \"\"\"\n        Initialize the Multi-LLM Analyzer.\n\n        Args:\n            api_key: OpenRouter API key (defaults to OPENROUTER_API_KEY env var)\n            models: List of models to use (defaults to all three models)\n            max_retries: Maximum retry attempts per request\n            timeout: Request timeout in seconds\n            rate_limit_delay: Delay between requests to avoid rate limits\n            use_async: Whether to use async client (recommended)\n        \"\"\"\n    self.api_key = api_key or os.getenv('OPENROUTER_API_KEY')\n    if not self.api_key:\n        raise ValueError('OpenRouter API key must be provided or set in OPENROUTER_API_KEY env var')\n    self.models = models or [LLMModel.GEMINI_3_PRO, LLMModel.CLAUDE_SONNET_4, LLMModel.GPT4O]\n    self.max_retries = max_retries\n    self.timeout = timeout\n    self.rate_limit_delay = rate_limit_delay\n    self.use_async = use_async\n    base_url = 'https://openrouter.ai/api/v1'\n    if use_async:\n        self.client = AsyncOpenAI(api_key=self.api_key, base_url=base_url, timeout=timeout)\n    else:\n        self.sync_client = OpenAI(api_key=self.api_key, base_url=base_url, timeout=timeout)\n    logger.info(f'Initialized MultiLLMAnalyzer with models: {[m.value for m in self.models]}')"
    },
    {
      "name": "_query_llm_async",
      "line": 177,
      "code": "async def _query_llm_async(self, model: LLMModel, prompt: str, system_prompt: Optional[str]=None, temperature: float=0.7, max_tokens: int=2000) -> LLMResponse:\n    \"\"\"\n        Query a single LLM with retry logic.\n\n        Args:\n            model: The LLM model to query\n            prompt: The user prompt\n            system_prompt: Optional system prompt\n            temperature: Sampling temperature\n            max_tokens: Maximum tokens in response\n\n        Returns:\n            LLMResponse object containing the result\n        \"\"\"\n    messages = []\n    if system_prompt:\n        messages.append({'role': 'system', 'content': system_prompt})\n    messages.append({'role': 'user', 'content': prompt})\n    for attempt in range(self.max_retries):\n        try:\n            start_time = time.time()\n            response = await self.client.chat.completions.create(model=model.value, messages=messages, temperature=temperature, max_tokens=max_tokens)\n            latency = time.time() - start_time\n            content = response.choices[0].message.content\n            tokens_used = response.usage.total_tokens if response.usage else 0\n            logger.debug(f'Successfully queried {model.value} in {latency:.2f}s')\n            return LLMResponse(model=model.value, content=content, tokens_used=tokens_used, latency=latency, success=True)\n        except Exception as e:\n            logger.warning(f'Attempt {attempt + 1}/{self.max_retries} failed for {model.value}: {str(e)}')\n            if attempt == self.max_retries - 1:\n                return LLMResponse(model=model.value, content='', tokens_used=0, latency=0, success=False, error=str(e))\n            await asyncio.sleep(2 ** attempt * self.rate_limit_delay)"
    },
    {
      "name": "_query_llm_sync",
      "line": 247,
      "code": "def _query_llm_sync(self, model: LLMModel, prompt: str, system_prompt: Optional[str]=None, temperature: float=0.7, max_tokens: int=2000) -> LLMResponse:\n    \"\"\"Synchronous version of _query_llm_async.\"\"\"\n    messages = []\n    if system_prompt:\n        messages.append({'role': 'system', 'content': system_prompt})\n    messages.append({'role': 'user', 'content': prompt})\n    for attempt in range(self.max_retries):\n        try:\n            start_time = time.time()\n            response = self.sync_client.chat.completions.create(model=model.value, messages=messages, temperature=temperature, max_tokens=max_tokens)\n            latency = time.time() - start_time\n            content = response.choices[0].message.content\n            tokens_used = response.usage.total_tokens if response.usage else 0\n            logger.debug(f'Successfully queried {model.value} in {latency:.2f}s')\n            return LLMResponse(model=model.value, content=content, tokens_used=tokens_used, latency=latency, success=True)\n        except Exception as e:\n            logger.warning(f'Attempt {attempt + 1}/{self.max_retries} failed for {model.value}: {str(e)}')\n            if attempt == self.max_retries - 1:\n                return LLMResponse(model=model.value, content='', tokens_used=0, latency=0, success=False, error=str(e))\n            time.sleep(2 ** attempt * self.rate_limit_delay)"
    },
    {
      "name": "_query_all_llms_async",
      "line": 305,
      "code": "async def _query_all_llms_async(self, prompt: str, system_prompt: Optional[str]=None, temperature: float=0.7, max_tokens: int=2000) -> List[LLMResponse]:\n    \"\"\"\n        Query all configured LLMs in parallel.\n\n        Args:\n            prompt: The user prompt\n            system_prompt: Optional system prompt\n            temperature: Sampling temperature\n            max_tokens: Maximum tokens in response\n\n        Returns:\n            List of LLMResponse objects\n        \"\"\"\n    tasks = [self._query_llm_async(model, prompt, system_prompt, temperature, max_tokens) for model in self.models]\n    responses = await asyncio.gather(*tasks, return_exceptions=True)\n    valid_responses = []\n    for i, response in enumerate(responses):\n        if isinstance(response, Exception):\n            logger.error(f'Error querying {self.models[i].value}: {str(response)}')\n            valid_responses.append(LLMResponse(model=self.models[i].value, content='', tokens_used=0, latency=0, success=False, error=str(response)))\n        else:\n            valid_responses.append(response)\n    return valid_responses"
    },
    {
      "name": "_query_all_llms_sync",
      "line": 351,
      "code": "def _query_all_llms_sync(self, prompt: str, system_prompt: Optional[str]=None, temperature: float=0.7, max_tokens: int=2000) -> List[LLMResponse]:\n    \"\"\"Synchronous version of _query_all_llms_async.\"\"\"\n    responses = []\n    for model in self.models:\n        response = self._query_llm_sync(model, prompt, system_prompt, temperature, max_tokens)\n        responses.append(response)\n        time.sleep(self.rate_limit_delay)\n    return responses"
    },
    {
      "name": "_parse_sentiment_score",
      "line": 368,
      "code": "def _parse_sentiment_score(self, content: str) -> Optional[float]:\n    \"\"\"\n        Parse sentiment score from LLM response.\n\n        Looks for patterns like:\n        - \"sentiment: 0.75\"\n        - \"score: -0.5\"\n        - JSON with \"sentiment\" or \"score\" field\n\n        Args:\n            content: LLM response content\n\n        Returns:\n            Sentiment score between -1.0 and 1.0, or None if not found\n        \"\"\"\n    try:\n        if content.strip().startswith('{'):\n            data = json.loads(content)\n            score = data.get('sentiment') or data.get('score')\n            if score is not None:\n                return max(-1.0, min(1.0, float(score)))\n        import re\n        patterns = ['sentiment[:\\\\s]+(-?\\\\d+\\\\.?\\\\d*)', 'score[:\\\\s]+(-?\\\\d+\\\\.?\\\\d*)', 'rating[:\\\\s]+(-?\\\\d+\\\\.?\\\\d*)']\n        for pattern in patterns:\n            match = re.search(pattern, content.lower())\n            if match:\n                score = float(match.group(1))\n                return max(-1.0, min(1.0, score))\n        return None\n    except Exception as e:\n        logger.warning(f'Error parsing sentiment score: {str(e)}')\n        return None"
    },
    {
      "name": "_parse_ipo_score",
      "line": 412,
      "code": "def _parse_ipo_score(self, content: str) -> Optional[int]:\n    \"\"\"\n        Parse IPO score from LLM response.\n\n        Args:\n            content: LLM response content\n\n        Returns:\n            IPO score between 0 and 100, or None if not found\n        \"\"\"\n    try:\n        if content.strip().startswith('{'):\n            data = json.loads(content)\n            score = data.get('ipo_score') or data.get('score')\n            if score is not None:\n                return max(0, min(100, int(score)))\n        import re\n        patterns = ['ipo[_\\\\s]+score[:\\\\s]+(\\\\d+)', 'score[:\\\\s]+(\\\\d+)', 'rating[:\\\\s]+(\\\\d+)']\n        for pattern in patterns:\n            match = re.search(pattern, content.lower())\n            if match:\n                score = int(match.group(1))\n                return max(0, min(100, score))\n        return None\n    except Exception as e:\n        logger.warning(f'Error parsing IPO score: {str(e)}')\n        return None"
    },
    {
      "name": "_calculate_ensemble_sentiment",
      "line": 451,
      "code": "def _calculate_ensemble_sentiment(self, responses: List[LLMResponse]) -> Tuple[float, float, Dict[str, float]]:\n    \"\"\"\n        Calculate ensemble sentiment from multiple LLM responses.\n\n        Args:\n            responses: List of LLMResponse objects\n\n        Returns:\n            Tuple of (ensemble_score, confidence, individual_scores)\n        \"\"\"\n    individual_scores = {}\n    valid_scores = []\n    for response in responses:\n        if response.success:\n            score = self._parse_sentiment_score(response.content)\n            if score is not None:\n                individual_scores[response.model] = score\n                valid_scores.append(score)\n    if not valid_scores:\n        logger.warning('No valid sentiment scores found in responses')\n        return (0.0, 0.0, individual_scores)\n    ensemble_score = sum(valid_scores) / len(valid_scores)\n    if len(valid_scores) > 1:\n        variance = sum(((s - ensemble_score) ** 2 for s in valid_scores)) / len(valid_scores)\n        confidence = max(0.0, min(1.0, 1.0 - variance))\n    else:\n        confidence = 0.5\n    return (ensemble_score, confidence, individual_scores)"
    },
    {
      "name": "_calculate_ensemble_ipo_score",
      "line": 492,
      "code": "def _calculate_ensemble_ipo_score(self, responses: List[LLMResponse]) -> Tuple[int, float, Dict[str, int]]:\n    \"\"\"\n        Calculate ensemble IPO score from multiple LLM responses.\n\n        Args:\n            responses: List of LLMResponse objects\n\n        Returns:\n            Tuple of (ensemble_score, confidence, individual_scores)\n        \"\"\"\n    individual_scores = {}\n    valid_scores = []\n    for response in responses:\n        if response.success:\n            score = self._parse_ipo_score(response.content)\n            if score is not None:\n                individual_scores[response.model] = score\n                valid_scores.append(score)\n    if not valid_scores:\n        logger.warning('No valid IPO scores found in responses')\n        return (50, 0.0, individual_scores)\n    ensemble_score = int(sum(valid_scores) / len(valid_scores))\n    if len(valid_scores) > 1:\n        variance = sum(((s - ensemble_score) ** 2 for s in valid_scores)) / len(valid_scores)\n        confidence = max(0.0, min(1.0, 1.0 - variance / 2500))\n    else:\n        confidence = 0.5\n    return (ensemble_score, confidence, individual_scores)"
    },
    {
      "name": "get_ensemble_sentiment",
      "line": 533,
      "code": "async def get_ensemble_sentiment(self, market_data: Dict[str, Any], news: List[Dict[str, Any]]) -> float:\n    \"\"\"\n        Generate ensemble sentiment score from market data and news.\n\n        Args:\n            market_data: Dictionary containing market data (prices, volume, indicators)\n            news: List of news articles with 'title', 'content', 'source', etc.\n\n        Returns:\n            Sentiment score between -1.0 (very bearish) and 1.0 (very bullish)\n\n        Example:\n            >>> market_data = {\n            ...     'symbol': 'AAPL',\n            ...     'price': 150.0,\n            ...     'change': 2.5,\n            ...     'volume': 1000000\n            ... }\n            >>> news = [\n            ...     {'title': 'Apple announces new product', 'content': '...'},\n            ...     {'title': 'Strong earnings beat', 'content': '...'}\n            ... ]\n            >>> sentiment = await analyzer.get_ensemble_sentiment(market_data, news)\n        \"\"\"\n    market_summary = json.dumps(market_data, indent=2)\n    news_summary = '\\n'.join([f\"- {article.get('title', 'N/A')}: {article.get('content', '')[:200]}...\" for article in news[:5]])\n    prompt = f'Analyze the following market data and news to provide a sentiment score.\\n\\nMarket Data:\\n{market_summary}\\n\\nRecent News:\\n{news_summary}\\n\\nProvide a sentiment score between -1.0 (very bearish) and 1.0 (very bullish).\\nInclude your reasoning.\\n\\nFormat your response as JSON:\\n{{\\n    \"sentiment\": <score>,\\n    \"reasoning\": \"<your analysis>\"\\n}}\\n'\n    system_prompt = 'You are an expert financial analyst specializing in market sentiment analysis.\\nProvide objective, data-driven sentiment scores based on technical indicators and news sentiment.'\n    if self.use_async:\n        responses = await self._query_all_llms_async(prompt, system_prompt, temperature=0.3)\n    else:\n        responses = self._query_all_llms_sync(prompt, system_prompt, temperature=0.3)\n    ensemble_score, confidence, individual_scores = self._calculate_ensemble_sentiment(responses)\n    logger.info(f'Ensemble sentiment: {ensemble_score:.3f} (confidence: {confidence:.3f})')\n    logger.info(f'Individual scores: {individual_scores}')\n    return ensemble_score"
    },
    {
      "name": "get_ensemble_sentiment_detailed",
      "line": 609,
      "code": "async def get_ensemble_sentiment_detailed(self, market_data: Dict[str, Any], news: List[Dict[str, Any]]) -> SentimentAnalysis:\n    \"\"\"\n        Generate detailed ensemble sentiment analysis.\n\n        Args:\n            market_data: Dictionary containing market data\n            news: List of news articles\n\n        Returns:\n            SentimentAnalysis object with detailed results\n        \"\"\"\n    market_summary = json.dumps(market_data, indent=2)\n    news_summary = '\\n'.join([f\"- {article.get('title', 'N/A')}: {article.get('content', '')[:200]}...\" for article in news[:5]])\n    prompt = f'Analyze the following market data and news to provide a detailed sentiment analysis.\\n\\nMarket Data:\\n{market_summary}\\n\\nRecent News:\\n{news_summary}\\n\\nProvide a comprehensive sentiment analysis.\\n\\nFormat your response as JSON:\\n{{\\n    \"sentiment\": <score between -1.0 and 1.0>,\\n    \"reasoning\": \"<detailed analysis>\"\\n}}\\n'\n    system_prompt = 'You are an expert financial analyst specializing in market sentiment analysis.\\nProvide objective, data-driven sentiment scores with detailed reasoning.'\n    if self.use_async:\n        responses = await self._query_all_llms_async(prompt, system_prompt, temperature=0.3)\n    else:\n        responses = self._query_all_llms_sync(prompt, system_prompt, temperature=0.3)\n    ensemble_score, confidence, individual_scores = self._calculate_ensemble_sentiment(responses)\n    reasoning_parts = []\n    for response in responses:\n        if response.success and response.content:\n            try:\n                data = json.loads(response.content)\n                if 'reasoning' in data:\n                    reasoning_parts.append(f\"[{response.model}] {data['reasoning']}\")\n            except Exception:\n                pass\n    reasoning = '\\n\\n'.join(reasoning_parts) if reasoning_parts else 'No detailed reasoning available'\n    return SentimentAnalysis(score=ensemble_score, confidence=confidence, reasoning=reasoning, individual_scores=individual_scores, metadata={'market_data': market_data, 'news_count': len(news), 'timestamp': time.time()})"
    },
    {
      "name": "analyze_ipo",
      "line": 694,
      "code": "async def analyze_ipo(self, company_data: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"\n        Analyze an IPO opportunity and provide scoring.\n\n        Args:\n            company_data: Dictionary containing company information:\n                - name: Company name\n                - sector: Industry sector\n                - financials: Financial metrics\n                - description: Company description\n                - ipo_details: IPO pricing and timing details\n\n        Returns:\n            Dictionary containing:\n                - score: IPO score (0-100)\n                - recommendation: Investment recommendation\n                - risk_level: Risk assessment\n                - key_factors: List of positive factors\n                - concerns: List of concerns\n                - confidence: Confidence level (0.0-1.0)\n\n        Example:\n            >>> company_data = {\n            ...     'name': 'TechCorp',\n            ...     'sector': 'Technology',\n            ...     'financials': {'revenue': 1000000, 'growth_rate': 0.5},\n            ...     'ipo_details': {'price_range': '15-17', 'date': '2025-11-01'}\n            ... }\n            >>> result = await analyzer.analyze_ipo(company_data)\n        \"\"\"\n    company_summary = json.dumps(company_data, indent=2)\n    prompt = f'Analyze the following IPO opportunity and provide a comprehensive assessment.\\n\\nCompany Data:\\n{company_summary}\\n\\nProvide a detailed IPO analysis including:\\n1. Overall score (0-100)\\n2. Investment recommendation (Strong Buy, Buy, Hold, Avoid)\\n3. Risk level (Low, Medium, High)\\n4. Key positive factors\\n5. Concerns and risks\\n6. Price target if applicable\\n\\nFormat your response as JSON:\\n{{\\n    \"score\": <0-100>,\\n    \"recommendation\": \"<recommendation>\",\\n    \"risk_level\": \"<risk level>\",\\n    \"key_factors\": [\"factor1\", \"factor2\", ...],\\n    \"concerns\": [\"concern1\", \"concern2\", ...],\\n    \"price_target\": <number or null>,\\n    \"analysis\": \"<detailed analysis>\"\\n}}\\n'\n    system_prompt = 'You are an expert IPO analyst with deep experience evaluating pre-IPO companies.\\nProvide thorough, objective analysis considering market conditions, company fundamentals,\\nvaluation, competitive landscape, and risk factors.'\n    if self.use_async:\n        responses = await self._query_all_llms_async(prompt, system_prompt, temperature=0.3)\n    else:\n        responses = self._query_all_llms_sync(prompt, system_prompt, temperature=0.3)\n    ensemble_score, confidence, individual_scores = self._calculate_ensemble_ipo_score(responses)\n    all_factors = []\n    all_concerns = []\n    all_recommendations = []\n    all_risk_levels = []\n    price_targets = []\n    individual_analyses = {}\n    for response in responses:\n        if response.success and response.content:\n            try:\n                data = json.loads(response.content)\n                individual_analyses[response.model] = data\n                if 'key_factors' in data:\n                    all_factors.extend(data['key_factors'])\n                if 'concerns' in data:\n                    all_concerns.extend(data['concerns'])\n                if 'recommendation' in data:\n                    all_recommendations.append(data['recommendation'])\n                if 'risk_level' in data:\n                    all_risk_levels.append(data['risk_level'])\n                if 'price_target' in data and data['price_target']:\n                    price_targets.append(data['price_target'])\n            except Exception as e:\n                logger.warning(f'Error parsing IPO analysis from {response.model}: {str(e)}')\n    if ensemble_score >= 80:\n        recommendation = 'Strong Buy'\n    elif ensemble_score >= 65:\n        recommendation = 'Buy'\n    elif ensemble_score >= 50:\n        recommendation = 'Hold'\n    else:\n        recommendation = 'Avoid'\n    risk_counts = {}\n    for risk in all_risk_levels:\n        risk_counts[risk] = risk_counts.get(risk, 0) + 1\n    risk_level = max(risk_counts.items(), key=lambda x: x[1])[0] if risk_counts else 'Medium'\n    avg_price_target = sum(price_targets) / len(price_targets) if price_targets else None\n    return {'score': ensemble_score, 'recommendation': recommendation, 'risk_level': risk_level, 'key_factors': list(set(all_factors))[:10], 'concerns': list(set(all_concerns))[:10], 'price_target': avg_price_target, 'confidence': confidence, 'individual_scores': individual_scores, 'individual_analyses': individual_analyses, 'timestamp': time.time()}"
    },
    {
      "name": "analyze_stock",
      "line": 833,
      "code": "async def analyze_stock(self, symbol: str, data: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"\n        Analyze a stock and provide trading recommendations.\n\n        Args:\n            symbol: Stock ticker symbol\n            data: Dictionary containing:\n                - price_data: Historical price data\n                - technical_indicators: Technical analysis indicators\n                - fundamentals: Fundamental metrics\n                - news: Recent news articles\n\n        Returns:\n            Dictionary containing analysis results\n\n        Example:\n            >>> data = {\n            ...     'price_data': {'current': 150, 'high_52w': 180, 'low_52w': 120},\n            ...     'technical_indicators': {'rsi': 65, 'macd': 'bullish'},\n            ...     'news': [...]\n            ... }\n            >>> result = await analyzer.analyze_stock('AAPL', data)\n        \"\"\"\n    data_summary = json.dumps(data, indent=2)\n    prompt = f'Analyze the following stock and provide trading recommendations.\\n\\nSymbol: {symbol}\\n\\nData:\\n{data_summary}\\n\\nProvide a comprehensive stock analysis including:\\n1. Sentiment score (-1.0 to 1.0)\\n2. Recommendation (Strong Buy, Buy, Hold, Sell, Strong Sell)\\n3. Target price\\n4. Risk assessment\\n5. Key insights\\n\\nFormat your response as JSON:\\n{{\\n    \"sentiment\": <-1.0 to 1.0>,\\n    \"recommendation\": \"<recommendation>\",\\n    \"target_price\": <number or null>,\\n    \"risk_assessment\": \"<assessment>\",\\n    \"key_insights\": [\"insight1\", \"insight2\", ...],\\n    \"analysis\": \"<detailed analysis>\"\\n}}\\n'\n    system_prompt = 'You are an expert stock analyst with expertise in technical analysis,\\nfundamental analysis, and market sentiment. Provide objective, actionable recommendations.'\n    if self.use_async:\n        responses = await self._query_all_llms_async(prompt, system_prompt, temperature=0.3)\n    else:\n        responses = self._query_all_llms_sync(prompt, system_prompt, temperature=0.3)\n    ensemble_sentiment, confidence, individual_scores = self._calculate_ensemble_sentiment(responses)\n    all_insights = []\n    all_recommendations = []\n    target_prices = []\n    for response in responses:\n        if response.success and response.content:\n            try:\n                data = json.loads(response.content)\n                if 'key_insights' in data:\n                    all_insights.extend(data['key_insights'])\n                if 'recommendation' in data:\n                    all_recommendations.append(data['recommendation'])\n                if 'target_price' in data and data['target_price']:\n                    target_prices.append(data['target_price'])\n            except Exception as e:\n                logger.warning(f'Error parsing stock analysis: {str(e)}')\n    if ensemble_sentiment >= 0.6:\n        recommendation = 'Strong Buy'\n    elif ensemble_sentiment >= 0.2:\n        recommendation = 'Buy'\n    elif ensemble_sentiment >= -0.2:\n        recommendation = 'Hold'\n    elif ensemble_sentiment >= -0.6:\n        recommendation = 'Sell'\n    else:\n        recommendation = 'Strong Sell'\n    avg_target = sum(target_prices) / len(target_prices) if target_prices else None\n    return {'symbol': symbol, 'sentiment': ensemble_sentiment, 'recommendation': recommendation, 'target_price': avg_target, 'risk_assessment': 'High' if abs(ensemble_sentiment) > 0.7 else 'Medium' if abs(ensemble_sentiment) > 0.3 else 'Low', 'key_insights': list(set(all_insights))[:10], 'confidence': confidence, 'individual_scores': individual_scores, 'timestamp': time.time()}"
    },
    {
      "name": "get_market_outlook",
      "line": 947,
      "code": "async def get_market_outlook(self) -> Dict[str, Any]:\n    \"\"\"\n        Generate overall market outlook and sentiment.\n\n        Returns:\n            Dictionary containing:\n                - overall_sentiment: Market sentiment score (-1.0 to 1.0)\n                - trend: Market trend (Bullish, Bearish, Neutral)\n                - key_drivers: List of key market drivers\n                - risks: List of market risks\n                - opportunities: List of opportunities\n                - timeframe: Analysis timeframe\n                - confidence: Confidence level\n\n        Example:\n            >>> outlook = await analyzer.get_market_outlook()\n            >>> print(f\"Market trend: {outlook['trend']}\")\n        \"\"\"\n    prompt = 'Provide a comprehensive outlook on the current market conditions.\\n\\nAnalyze:\\n1. Overall market sentiment and trend\\n2. Key drivers influencing the market\\n3. Major risks and concerns\\n4. Investment opportunities\\n5. Short-term and medium-term outlook\\n\\nFormat your response as JSON:\\n{\\n    \"sentiment\": <-1.0 to 1.0>,\\n    \"trend\": \"<Bullish/Bearish/Neutral>\",\\n    \"key_drivers\": [\"driver1\", \"driver2\", ...],\\n    \"risks\": [\"risk1\", \"risk2\", ...],\\n    \"opportunities\": [\"opp1\", \"opp2\", ...],\\n    \"short_term_outlook\": \"<outlook>\",\\n    \"medium_term_outlook\": \"<outlook>\",\\n    \"analysis\": \"<detailed analysis>\"\\n}\\n'\n    system_prompt = 'You are a senior market strategist with expertise in macroeconomic analysis,\\nmarket trends, and investment strategy. Provide comprehensive, balanced market outlook.'\n    if self.use_async:\n        responses = await self._query_all_llms_async(prompt, system_prompt, temperature=0.5)\n    else:\n        responses = self._query_all_llms_sync(prompt, system_prompt, temperature=0.5)\n    ensemble_sentiment, confidence, individual_scores = self._calculate_ensemble_sentiment(responses)\n    all_drivers = []\n    all_risks = []\n    all_opportunities = []\n    all_trends = []\n    for response in responses:\n        if response.success and response.content:\n            try:\n                data = json.loads(response.content)\n                if 'key_drivers' in data:\n                    all_drivers.extend(data['key_drivers'])\n                if 'risks' in data:\n                    all_risks.extend(data['risks'])\n                if 'opportunities' in data:\n                    all_opportunities.extend(data['opportunities'])\n                if 'trend' in data:\n                    all_trends.append(data['trend'])\n            except Exception as e:\n                logger.warning(f'Error parsing market outlook: {str(e)}')\n    if ensemble_sentiment >= 0.3:\n        trend = 'Bullish'\n    elif ensemble_sentiment <= -0.3:\n        trend = 'Bearish'\n    else:\n        trend = 'Neutral'\n    return {'overall_sentiment': ensemble_sentiment, 'trend': trend, 'key_drivers': list(set(all_drivers))[:8], 'risks': list(set(all_risks))[:8], 'opportunities': list(set(all_opportunities))[:8], 'timeframe': 'Short to Medium Term (1-6 months)', 'confidence': confidence, 'individual_scores': individual_scores, 'timestamp': time.time()}"
    },
    {
      "name": "close",
      "line": 1044,
      "code": "def close(self):\n    \"\"\"Close the client connections.\"\"\"\n    if hasattr(self, 'client') and self.client:\n        pass\n    logger.info('MultiLLMAnalyzer closed')"
    },
    {
      "name": "main",
      "line": 1073,
      "code": "async def main():\n    analyzer = MultiLLMAnalyzer()\n    market_data = {'symbol': 'AAPL', 'price': 150.0, 'change_percent': 2.5, 'volume': 50000000, 'rsi': 65, 'macd': 'bullish'}\n    news = [{'title': 'Apple announces new AI features', 'content': 'Apple unveiled significant AI capabilities...', 'source': 'TechNews', 'sentiment': 'positive'}, {'title': 'Strong quarterly earnings reported', 'content': 'Apple beat expectations with...', 'source': 'FinanceDaily', 'sentiment': 'positive'}]\n    sentiment = await analyzer.get_ensemble_sentiment(market_data, news)\n    print(f'Ensemble Sentiment: {sentiment:.3f}')\n    detailed = await analyzer.get_ensemble_sentiment_detailed(market_data, news)\n    print('\\nDetailed Analysis:')\n    print(f'Score: {detailed.score:.3f}')\n    print(f'Confidence: {detailed.confidence:.3f}')\n    print(f'Individual Scores: {detailed.individual_scores}')\n    ipo_data = {'name': 'TechCorp', 'sector': 'Technology', 'financials': {'revenue': 500000000, 'growth_rate': 0.45, 'profit_margin': 0.15}, 'description': 'Leading AI software company', 'ipo_details': {'price_range': '15-17', 'shares': 10000000, 'date': '2025-11-15'}}\n    ipo_analysis = await analyzer.analyze_ipo(ipo_data)\n    print('\\nIPO Analysis:')\n    print(f\"Score: {ipo_analysis['score']}/100\")\n    print(f\"Recommendation: {ipo_analysis['recommendation']}\")\n    print(f\"Risk Level: {ipo_analysis['risk_level']}\")\n    outlook = await analyzer.get_market_outlook()\n    print('\\nMarket Outlook:')\n    print(f\"Trend: {outlook['trend']}\")\n    print(f\"Sentiment: {outlook['overall_sentiment']:.3f}\")\n    print(f\"Key Drivers: {outlook['key_drivers'][:3]}\")\n    analyzer.close()"
    }
  ]
}
