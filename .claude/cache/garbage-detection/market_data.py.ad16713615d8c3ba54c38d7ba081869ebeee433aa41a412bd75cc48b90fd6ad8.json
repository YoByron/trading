{
  "dead_imports": [
    {
      "line": 14,
      "module": "annotations",
      "statement": "from __future__ import annotations"
    }
  ],
  "commented_blocks": [],
  "empty_functions": [],
  "unused_variables": [
    {
      "line": 36,
      "variable": "ALPACA"
    },
    {
      "line": 37,
      "variable": "POLYGON"
    },
    {
      "line": 38,
      "variable": "CACHE"
    },
    {
      "line": 39,
      "variable": "YFINANCE"
    },
    {
      "line": 40,
      "variable": "ALPHA_VANTAGE"
    },
    {
      "line": 41,
      "variable": "UNKNOWN"
    }
  ],
  "all_functions": [
    {
      "name": "get_market_data_provider",
      "line": 1367,
      "code": "def get_market_data_provider() -> MarketDataProvider:\n    \"\"\"Convenience singleton-style accessor.\"\"\"\n    if not hasattr(get_market_data_provider, '_instance'):\n        get_market_data_provider._instance = MarketDataProvider()\n    return get_market_data_provider._instance"
    },
    {
      "name": "update",
      "line": 68,
      "code": "def update(self, success: bool, latency_ms: float):\n    \"\"\"Update metrics with a new request.\"\"\"\n    self.total_requests += 1\n    if success:\n        self.successful_requests += 1\n    else:\n        self.failed_requests += 1\n    self.total_latency_ms += latency_ms\n    self.avg_latency_ms = self.total_latency_ms / self.total_requests\n    self.success_rate = self.successful_requests / self.total_requests if self.total_requests > 0 else 0.0"
    },
    {
      "name": "add_attempt",
      "line": 95,
      "code": "def add_attempt(self, attempt: FetchAttempt) -> None:\n    \"\"\"Track a fetch attempt.\"\"\"\n    self.attempts.append(attempt)\n    self.total_attempts += 1\n    self.total_latency_ms += attempt.latency_ms"
    },
    {
      "name": "to_dict",
      "line": 101,
      "code": "def to_dict(self) -> dict:\n    \"\"\"Convert to dictionary for logging/reporting.\"\"\"\n    return {'source': self.source.value, 'rows': len(self.data), 'total_attempts': self.total_attempts, 'total_latency_ms': round(self.total_latency_ms, 2), 'cache_age_hours': round(self.cache_age_hours, 2) if self.cache_age_hours else None, 'attempts': [{'source': a.source.value, 'success': a.success, 'error': a.error_message, 'rows': a.rows_fetched, 'latency_ms': round(a.latency_ms, 2)} for a in self.attempts]}"
    },
    {
      "name": "__init__",
      "line": 127,
      "code": "def __init__(self, alpha_vantage_key: Optional[str]=None, session: Optional[requests.Session]=None) -> None:\n    self.YFINANCE_LOOKBACK_BUFFER_DAYS = int(os.getenv('YFINANCE_LOOKBACK_BUFFER_DAYS', '35'))\n    self.YFINANCE_SECONDARY_LOOKBACK_DAYS = int(os.getenv('YFINANCE_SECONDARY_LOOKBACK_DAYS', '150'))\n    self.POLYGON_LOOKBACK_BUFFER_DAYS = int(os.getenv('POLYGON_LOOKBACK_BUFFER_DAYS', '45'))\n    self.YFINANCE_MAX_RETRIES = int(os.getenv('YFINANCE_MAX_RETRIES', '3'))\n    self.YFINANCE_INITIAL_BACKOFF_SECONDS = float(os.getenv('YFINANCE_INITIAL_BACKOFF_SECONDS', '1.0'))\n    self.ALPACA_MAX_RETRIES = int(os.getenv('ALPACA_MAX_RETRIES', '3'))\n    self.ALPACA_INITIAL_BACKOFF_SECONDS = float(os.getenv('ALPACA_INITIAL_BACKOFF_SECONDS', '2.0'))\n    self.ALPHAVANTAGE_MIN_INTERVAL_SECONDS = float(os.getenv('ALPHAVANTAGE_MIN_INTERVAL_SECONDS', '15'))\n    self.ALPHAVANTAGE_BACKOFF_SECONDS = float(os.getenv('ALPHAVANTAGE_BACKOFF_SECONDS', '60'))\n    self.ALPHAVANTAGE_MAX_RETRIES = int(os.getenv('ALPHAVANTAGE_MAX_RETRIES', '4'))\n    self.ALPHAVANTAGE_MAX_TOTAL_SECONDS = float(os.getenv('ALPHAVANTAGE_MAX_TOTAL_SECONDS', '90'))\n    self.CACHE_TTL_SECONDS = int(os.getenv('CACHE_TTL_SECONDS', str(6 * 60 * 60)))\n    self.CACHE_MAX_AGE_DAYS = int(os.getenv('CACHE_MAX_AGE_DAYS', '7'))\n    self.MAX_DATA_AGE_HOURS = int(os.getenv('MAX_DATA_AGE_HOURS', '48'))\n    self.POLYGON_MAX_RETRIES = int(os.getenv('POLYGON_MAX_RETRIES', '3'))\n    self.POLYGON_INITIAL_BACKOFF_SECONDS = float(os.getenv('POLYGON_INITIAL_BACKOFF_SECONDS', '30.0'))\n    self.session = session or requests.Session()\n    self.session.headers.update({'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36', 'Accept': 'application/json,text/html;q=0.9,*/*;q=0.8'})\n    self.alpha_vantage_key = alpha_vantage_key or os.getenv('ALPHA_VANTAGE_API_KEY')\n    self._last_alpha_call_ts: float = 0.0\n    self._cache: Dict[Tuple[str, int, date], pd.DataFrame] = {}\n    cache_root = os.getenv('MARKET_DATA_CACHE_DIR', 'data/cache/alpha_vantage')\n    self.cache_dir = Path(cache_root)\n    self.cache_dir.mkdir(parents=True, exist_ok=True)\n    self._health_log_file = self.cache_dir / 'health_log.jsonl'\n    self._metrics: Dict[DataSource, PerformanceMetrics] = {source: PerformanceMetrics(source=source) for source in DataSource}\n    self._alpaca_api = None\n    alpaca_key = os.getenv('ALPACA_API_KEY')\n    alpaca_secret = os.getenv('ALPACA_SECRET_KEY')\n    if alpaca_key and alpaca_secret:\n        try:\n            import alpaca_trade_api as tradeapi\n            self._alpaca_api = tradeapi.REST(key_id=alpaca_key, secret_key=alpaca_secret, base_url=os.getenv('APCA_API_BASE_URL', 'https://paper-api.alpaca.markets'), api_version='v2')\n            logger.info('\u2705 Alpaca API initialized as PRIMARY market data source (most reliable)')\n        except Exception as e:\n            logger.warning(f'Failed to initialize Alpaca API for market data: {e}')\n            self._alpaca_api = None\n    self.polygon_api_key = os.getenv('POLYGON_API_KEY')\n    if self.polygon_api_key:\n        logger.info('\u2705 Polygon.io API available as secondary market data source')\n    else:\n        logger.debug('Polygon.io API not configured (optional)')\n    self._log_configuration()"
    },
    {
      "name": "get_performance_metrics",
      "line": 240,
      "code": "def get_performance_metrics(self) -> Dict[str, Dict[str, float]]:\n    \"\"\"Get performance metrics for all data sources.\"\"\"\n    return {source.value: {'total_requests': metrics.total_requests, 'successful_requests': metrics.successful_requests, 'failed_requests': metrics.failed_requests, 'success_rate': metrics.success_rate, 'avg_latency_ms': metrics.avg_latency_ms} for source, metrics in self._metrics.items() if metrics.total_requests > 0}"
    },
    {
      "name": "get_performance_metrics",
      "line": 254,
      "code": "def get_performance_metrics(self) -> Dict[str, Dict[str, float]]:\n    \"\"\"Get performance metrics for all data sources.\"\"\"\n    return {source.value: {'total_requests': metrics.total_requests, 'successful_requests': metrics.successful_requests, 'failed_requests': metrics.failed_requests, 'success_rate': metrics.success_rate, 'avg_latency_ms': metrics.avg_latency_ms} for source, metrics in self._metrics.items() if metrics.total_requests > 0}"
    },
    {
      "name": "get_daily_bars",
      "line": 268,
      "code": "def get_daily_bars(self, symbol: str, lookback_days: int, end_datetime: Optional[datetime]=None) -> MarketDataResult:\n    \"\"\"\n        Retrieve daily OHLCV candles for a symbol with comprehensive fallback tracking.\n\n        Args:\n            symbol: Equity ticker symbol.\n            lookback_days: Number of trading days required (excludes buffer).\n            end_datetime: Optional custom end date (defaults to now).\n\n        Returns:\n            MarketDataResult with data and metadata about fetch attempts.\n\n        Raises:\n            ValueError: if data cannot be fetched from any source.\n        \"\"\"\n    end_dt = end_datetime or datetime.now()\n    start_dt = end_dt - timedelta(days=lookback_days + self.YFINANCE_LOOKBACK_BUFFER_DAYS)\n    cache_key = (symbol.upper(), lookback_days, end_dt.date())\n    result = MarketDataResult(data=pd.DataFrame(), source=DataSource.UNKNOWN)\n    cached = self._cache.get(cache_key)\n    if cached is not None and (not cached.empty):\n        logger.debug('%s: Serving from in-memory cache', symbol)\n        result.data = cached.copy()\n        result.source = DataSource.CACHE\n        return result\n    if self.polygon_api_key:\n        logger.info('%s: Fetching from Polygon.io API (primary reliable source)', symbol)\n        data = self._fetch_polygon_with_retries(symbol, lookback_days, result)\n        if self._is_valid(data, lookback_days):\n            prepared = self._prepare(data, lookback_days)\n            self._cache[cache_key] = prepared\n            result.data = prepared.copy()\n            result.source = DataSource.POLYGON\n            self._log_health(symbol, result)\n            self._metrics[DataSource.POLYGON].update(True, result.total_latency_ms)\n            self._cache_polygon_response(symbol, prepared)\n            logger.info('%s: \u2705 Successfully fetched from Polygon.io (%d rows, %d attempts, %.2fms)', symbol, len(prepared), result.total_attempts, result.total_latency_ms)\n            return result\n        else:\n            logger.warning('%s: Polygon.io returned insufficient data, trying Alpaca', symbol)\n    if self._alpaca_api:\n        logger.info('%s: Fetching from Alpaca API (secondary reliable source)', symbol)\n        data = self._fetch_alpaca_with_retries(symbol, lookback_days, result)\n        if self._is_valid(data, lookback_days):\n            prepared = self._prepare(data, lookback_days)\n            self._cache[cache_key] = prepared\n            result.data = prepared.copy()\n            result.source = DataSource.ALPACA\n            self._log_health(symbol, result)\n            self._metrics[DataSource.ALPACA].update(True, result.total_latency_ms)\n            logger.info('%s: \u2705 Successfully fetched from Alpaca API (%d rows, %d attempts, %.2fms)', symbol, len(prepared), result.total_attempts, result.total_latency_ms)\n            return result\n        else:\n            logger.warning('%s: Alpaca API returned insufficient data, trying cache/yfinance', symbol)\n    cached_data, cache_age_hours = self._load_cached_data_with_age(symbol, lookback_days)\n    if cached_data is not None and cache_age_hours is not None and (cache_age_hours <= self.MAX_DATA_AGE_HOURS):\n        logger.info('%s: Using cached data (%.1f hours old, max allowed: %d hours) - reliable fallback', symbol, cache_age_hours, self.MAX_DATA_AGE_HOURS)\n        result.data = cached_data\n        result.source = DataSource.CACHE\n        result.cache_age_hours = cache_age_hours\n        self._log_health(symbol, result)\n        return result\n    logger.warning('%s: Paid sources unavailable/unreliable. Trying yfinance (unreliable free source).', symbol)\n    data = self._fetch_yfinance_with_retries(symbol, start_dt, end_dt, result)\n    if self._is_valid(data, lookback_days):\n        prepared = self._prepare(data, lookback_days)\n        self._cache[cache_key] = prepared\n        result.data = prepared.copy()\n        result.source = DataSource.YFINANCE\n        self._log_health(symbol, result)\n        logger.info('%s: Successfully fetched from yfinance (%d rows, %d attempts, %.2fms)', symbol, len(prepared), result.total_attempts, result.total_latency_ms)\n        return result\n    cached_data, cache_age_hours = self._load_cached_data_with_age(symbol, lookback_days)\n    if cached_data is not None and cache_age_hours is not None and (cache_age_hours <= self.MAX_DATA_AGE_HOURS):\n        logger.info('%s: Using cached data (%.1f hours old, max allowed: %d hours) before trying Alpha Vantage', symbol, cache_age_hours, self.MAX_DATA_AGE_HOURS)\n        result.data = cached_data\n        result.source = DataSource.CACHE\n        result.cache_age_hours = cache_age_hours\n        self._log_health(symbol, result)\n        return result\n    if not self.alpha_vantage_key:\n        logger.warning('%s: Alpha Vantage fallback unavailable (ALPHA_VANTAGE_API_KEY not configured).', symbol)\n    else:\n        logger.warning('%s: Alpaca API failed. Trying Alpha Vantage (will fail fast if rate-limited).', symbol)\n        data = self._fetch_alpha_vantage_with_retries(symbol, result)\n        if self._is_valid(data, lookback_days):\n            prepared = self._prepare(data, lookback_days)\n            self._cache[cache_key] = prepared\n            result.data = prepared.copy()\n            result.source = DataSource.ALPHA_VANTAGE\n            self._log_health(symbol, result)\n            logger.info('%s: Successfully fetched from Alpha Vantage (%d rows, %d attempts, %.2fms)', symbol, len(prepared), result.total_attempts, result.total_latency_ms)\n            return result\n    logger.warning('%s: All live data sources failed. Attempting to use cached data.', symbol)\n    cached_data, cache_age_hours = self._load_cached_data_with_age(symbol, lookback_days)\n    if cached_data is not None:\n        result.data = cached_data\n        result.source = DataSource.CACHE\n        result.cache_age_hours = cache_age_hours\n        self._log_health(symbol, result)\n        logger.warning('%s: Using cached data (%.1f hours old). Trading will proceed with caution.', symbol, cache_age_hours)\n        return result\n    self._log_health(symbol, result)\n    error_summary = '\\n'.join([f'  - {a.source.value}: {a.error_message}' for a in result.attempts if not a.success])\n    raise ValueError(f'Failed to fetch {lookback_days} days of data for {symbol} from all sources:\\n{error_summary}')"
    },
    {
      "name": "_log_configuration",
      "line": 487,
      "code": "def _log_configuration(self) -> None:\n    \"\"\"Log configuration at startup for debugging.\"\"\"\n    logger.info('MarketDataProvider configuration (RELIABLE FIRST):')\n    logger.info('  - Alpaca API: %s (PRIMARY - most reliable)', '\u2705 enabled' if self._alpaca_api else '\u274c disabled')\n    logger.info('  - Polygon.io: %s (SECONDARY - reliable paid)', '\u2705 enabled' if self.polygon_api_key else '\u274c disabled')\n    if self.polygon_api_key:\n        logger.info('    * Polygon retries: %d, backoff: %.0fs (exponential), max_data_age: %dh', self.POLYGON_MAX_RETRIES, self.POLYGON_INITIAL_BACKOFF_SECONDS, self.MAX_DATA_AGE_HOURS)\n    logger.info('  - Cache: dir=%s, ttl=%ds, max_age=%dd (FAST FALLBACK)', self.cache_dir, self.CACHE_TTL_SECONDS, self.CACHE_MAX_AGE_DAYS)\n    logger.info('  - yfinance: max_retries=%d (UNRELIABLE FREE - last resort)', self.YFINANCE_MAX_RETRIES)\n    logger.info('  - Alpha Vantage: %s (SLOW RATE-LIMITED - avoid)', 'enabled' if self.alpha_vantage_key else 'disabled')"
    },
    {
      "name": "_log_health",
      "line": 520,
      "code": "def _log_health(self, symbol: str, result: MarketDataResult) -> None:\n    \"\"\"Log fetch result to health log for monitoring.\"\"\"\n    try:\n        import json\n        from datetime import timezone\n        health_entry = {'timestamp': datetime.now(timezone.utc).isoformat(), 'symbol': symbol, **result.to_dict()}\n        with open(self._health_log_file, 'a') as f:\n            f.write(json.dumps(health_entry) + '\\n')\n    except Exception as exc:\n        logger.debug('Failed to write health log: %s', exc)"
    },
    {
      "name": "_fetch_yfinance_with_retries",
      "line": 539,
      "code": "def _fetch_yfinance_with_retries(self, symbol: str, start_dt: datetime, end_dt: datetime, result: MarketDataResult) -> Optional[pd.DataFrame]:\n    \"\"\"Fetch from yfinance with exponential backoff retries.\"\"\"\n    for attempt in range(1, self.YFINANCE_MAX_RETRIES + 1):\n        start_time = time.time()\n        try:\n            data = self._fetch_yfinance(symbol, start_dt, end_dt)\n            latency_ms = (time.time() - start_time) * 1000\n            if data is not None and (not data.empty):\n                result.add_attempt(FetchAttempt(source=DataSource.YFINANCE, timestamp=time.time(), success=True, rows_fetched=len(data), latency_ms=latency_ms))\n                return data\n            else:\n                result.add_attempt(FetchAttempt(source=DataSource.YFINANCE, timestamp=time.time(), success=False, error_message='Empty DataFrame returned', latency_ms=latency_ms))\n        except Exception as exc:\n            latency_ms = (time.time() - start_time) * 1000\n            result.add_attempt(FetchAttempt(source=DataSource.YFINANCE, timestamp=time.time(), success=False, error_message=str(exc), latency_ms=latency_ms))\n            logger.debug('%s: yfinance attempt %d/%d failed: %s', symbol, attempt, self.YFINANCE_MAX_RETRIES, exc)\n        if attempt < self.YFINANCE_MAX_RETRIES:\n            backoff = self.YFINANCE_INITIAL_BACKOFF_SECONDS * 2 ** (attempt - 1)\n            logger.debug('%s: Retrying yfinance in %.1fs...', symbol, backoff)\n            time.sleep(backoff)\n    return None"
    },
    {
      "name": "_fetch_alpaca_with_retries",
      "line": 600,
      "code": "def _fetch_alpaca_with_retries(self, symbol: str, lookback_days: int, result: MarketDataResult) -> Optional[pd.DataFrame]:\n    \"\"\"Fetch from Alpaca API with exponential backoff retries.\"\"\"\n    if not self._alpaca_api:\n        result.add_attempt(FetchAttempt(source=DataSource.ALPACA, timestamp=time.time(), success=False, error_message='Alpaca API not initialized (missing credentials)'))\n        return None\n    for attempt in range(1, self.ALPACA_MAX_RETRIES + 1):\n        start_time = time.time()\n        try:\n            data = self._fetch_alpaca(symbol, lookback_days)\n            latency_ms = (time.time() - start_time) * 1000\n            if data is not None and (not data.empty):\n                result.add_attempt(FetchAttempt(source=DataSource.ALPACA, timestamp=time.time(), success=True, rows_fetched=len(data), latency_ms=latency_ms))\n                return data\n            else:\n                result.add_attempt(FetchAttempt(source=DataSource.ALPACA, timestamp=time.time(), success=False, error_message='No bars returned', latency_ms=latency_ms))\n        except Exception as exc:\n            latency_ms = (time.time() - start_time) * 1000\n            result.add_attempt(FetchAttempt(source=DataSource.ALPACA, timestamp=time.time(), success=False, error_message=str(exc), latency_ms=latency_ms))\n            logger.debug('%s: Alpaca attempt %d/%d failed: %s', symbol, attempt, self.ALPACA_MAX_RETRIES, exc)\n        if attempt < self.ALPACA_MAX_RETRIES:\n            backoff = self.ALPACA_INITIAL_BACKOFF_SECONDS * 2 ** (attempt - 1)\n            logger.debug('%s: Retrying Alpaca API in %.1fs...', symbol, backoff)\n            time.sleep(backoff)\n    return None"
    },
    {
      "name": "_fetch_alpha_vantage_with_retries",
      "line": 668,
      "code": "def _fetch_alpha_vantage_with_retries(self, symbol: str, result: MarketDataResult) -> Optional[pd.DataFrame]:\n    \"\"\"\n        Fetch from Alpha Vantage with FAIL-FAST logic to avoid workflow timeouts.\n\n        CRITICAL: If rate-limited, we FAIL IMMEDIATELY instead of waiting 10+ minutes\n        for exponential backoff. This prevents GitHub Actions workflow timeouts.\n        \"\"\"\n    start_time = time.time()\n    max_total_time = self.ALPHAVANTAGE_MAX_TOTAL_SECONDS\n    try:\n        data = self._fetch_alpha_vantage(symbol, max_total_time=max_total_time, start_time=start_time)\n        latency_ms = (time.time() - start_time) * 1000\n        if data is not None and (not data.empty):\n            result.add_attempt(FetchAttempt(source=DataSource.ALPHA_VANTAGE, timestamp=time.time(), success=True, rows_fetched=len(data), latency_ms=latency_ms))\n            return data\n        else:\n            result.add_attempt(FetchAttempt(source=DataSource.ALPHA_VANTAGE, timestamp=time.time(), success=False, error_message='No time series data returned', latency_ms=latency_ms))\n    except TimeoutError as exc:\n        latency_ms = (time.time() - start_time) * 1000\n        result.add_attempt(FetchAttempt(source=DataSource.ALPHA_VANTAGE, timestamp=time.time(), success=False, error_message=f'Timeout after {latency_ms / 1000:.1f}s (max {max_total_time}s): {exc}', latency_ms=latency_ms))\n        logger.warning('%s: Alpha Vantage timed out after %.1fs (rate-limited). Using cached data instead.', symbol, latency_ms / 1000)\n    except Exception as exc:\n        latency_ms = (time.time() - start_time) * 1000\n        result.add_attempt(FetchAttempt(source=DataSource.ALPHA_VANTAGE, timestamp=time.time(), success=False, error_message=str(exc), latency_ms=latency_ms))\n        logger.debug('%s: Alpha Vantage failed: %s', symbol, exc)\n    return None"
    },
    {
      "name": "_fetch_yfinance",
      "line": 740,
      "code": "def _fetch_yfinance(self, symbol: str, start_dt: datetime, end_dt: datetime) -> Optional[pd.DataFrame]:\n    sleep_seconds = random.uniform(0.3, 1.2)\n    time.sleep(sleep_seconds)\n    try:\n        data = yf.download(symbol, start=start_dt, end=end_dt, progress=False, auto_adjust=False, threads=False)\n        if isinstance(data, pd.DataFrame) and (not data.empty):\n            return data\n        logger.debug('%s: yfinance primary download returned empty frame.', symbol)\n    except Exception as exc:\n        logger.warning('yfinance fetch failed for %s: %s', symbol, exc)\n        try:\n            from src.utils.error_monitoring import capture_data_source_failure\n            capture_data_source_failure('yfinance', symbol, str(exc))\n        except Exception:\n            pass\n    try:\n        ticker = yf.Ticker(symbol)\n        history = ticker.history(start=start_dt.strftime('%Y-%m-%d'), end=end_dt.strftime('%Y-%m-%d'), interval='1d', auto_adjust=False)\n        if isinstance(history, pd.DataFrame) and (not history.empty):\n            return history\n    except Exception as exc:\n        logger.debug('%s: yfinance ticker.history failed: %s', symbol, exc)\n    try:\n        extended_start = end_dt - timedelta(days=self.YFINANCE_SECONDARY_LOOKBACK_DAYS)\n        extended = yf.download(symbol, start=extended_start, end=end_dt, progress=False, auto_adjust=False, threads=False)\n        if isinstance(extended, pd.DataFrame) and (not extended.empty):\n            return extended\n    except Exception as exc:\n        logger.debug('%s: yfinance extended download failed: %s', symbol, exc)\n    return None"
    },
    {
      "name": "_fetch_alpaca",
      "line": 801,
      "code": "def _fetch_alpaca(self, symbol: str, lookback_days: int) -> Optional[pd.DataFrame]:\n    \"\"\"Fetch market data from Alpaca API (preferred fallback).\"\"\"\n    if not self._alpaca_api:\n        logger.debug('%s: Alpaca API not available (missing credentials)', symbol)\n        return None\n    try:\n        barset = self._alpaca_api.get_bars(symbol, '1Day', limit=lookback_days + self.YFINANCE_LOOKBACK_BUFFER_DAYS)\n        if not barset or len(barset) == 0:\n            logger.warning('%s: Alpaca API returned no bars', symbol)\n            return None\n        records = []\n        for bar in barset:\n            records.append({'Open': float(bar.o), 'High': float(bar.h), 'Low': float(bar.l), 'Close': float(bar.c), 'Volume': int(bar.v)})\n        if not records:\n            return None\n        df = pd.DataFrame(records, index=[bar.t for bar in barset])\n        df.index.name = 'Date'\n        df = df.sort_index()\n        logger.info('%s: Successfully fetched %d bars from Alpaca API', symbol, len(df))\n        return df\n    except Exception as exc:\n        logger.warning('%s: Alpaca API fetch failed: %s', symbol, exc)\n        return None"
    },
    {
      "name": "_fetch_polygon_with_retries",
      "line": 849,
      "code": "def _fetch_polygon_with_retries(self, symbol: str, lookback_days: int, result: MarketDataResult) -> Optional[pd.DataFrame]:\n    \"\"\"Fetch from Polygon.io API with exponential backoff retries and cache fallback.\"\"\"\n    if not self.polygon_api_key:\n        result.add_attempt(FetchAttempt(source=DataSource.POLYGON, timestamp=time.time(), success=False, error_message='Polygon.io API not configured (missing POLYGON_API_KEY)'))\n        return None\n    for attempt in range(1, self.POLYGON_MAX_RETRIES + 1):\n        start_time = time.time()\n        try:\n            data = self._fetch_polygon(symbol, lookback_days)\n            latency_ms = (time.time() - start_time) * 1000\n            if data is not None and (not data.empty):\n                result.add_attempt(FetchAttempt(source=DataSource.POLYGON, timestamp=time.time(), success=True, rows_fetched=len(data), latency_ms=latency_ms))\n                self._cache_polygon_response(symbol, data)\n                return data\n            else:\n                result.add_attempt(FetchAttempt(source=DataSource.POLYGON, timestamp=time.time(), success=False, error_message='No bars returned', latency_ms=latency_ms))\n        except requests.exceptions.HTTPError as exc:\n            latency_ms = (time.time() - start_time) * 1000\n            response = getattr(exc, 'response', None)\n            if response is not None and response.status_code == 429:\n                logger.warning('%s: Polygon.io rate limit hit (429) on attempt %d/%d', symbol, attempt, self.POLYGON_MAX_RETRIES)\n                result.add_attempt(FetchAttempt(source=DataSource.POLYGON, timestamp=time.time(), success=False, error_message=f'Rate limit (429) on attempt {attempt}', latency_ms=latency_ms))\n                if attempt < self.POLYGON_MAX_RETRIES:\n                    cached_data, cache_age_hours = self._load_cached_data_with_age(symbol, lookback_days)\n                    if cached_data is not None and cache_age_hours is not None:\n                        max_age_hours = self.MAX_DATA_AGE_HOURS\n                        if cache_age_hours <= max_age_hours:\n                            logger.info('%s: Using cached Polygon data (%.1f hours old) after 429', symbol, cache_age_hours)\n                            result.add_attempt(FetchAttempt(source=DataSource.CACHE, timestamp=time.time(), success=True, rows_fetched=len(cached_data), latency_ms=0.0))\n                            return cached_data\n                backoff = self.POLYGON_INITIAL_BACKOFF_SECONDS * 2 ** (attempt - 1)\n                logger.info('%s: Waiting %.0fs before retry (exponential backoff)', symbol, backoff)\n                time.sleep(backoff)\n            else:\n                result.add_attempt(FetchAttempt(source=DataSource.POLYGON, timestamp=time.time(), success=False, error_message=str(exc), latency_ms=latency_ms))\n                logger.debug('%s: Polygon.io attempt %d/%d failed: %s', symbol, attempt, self.POLYGON_MAX_RETRIES, exc)\n                if attempt < self.POLYGON_MAX_RETRIES:\n                    time.sleep(1)\n        except Exception as exc:\n            latency_ms = (time.time() - start_time) * 1000\n            result.add_attempt(FetchAttempt(source=DataSource.POLYGON, timestamp=time.time(), success=False, error_message=str(exc), latency_ms=latency_ms))\n            logger.debug('%s: Polygon.io attempt %d/%d failed: %s', symbol, attempt, self.POLYGON_MAX_RETRIES, exc)\n            if attempt < self.POLYGON_MAX_RETRIES:\n                time.sleep(1)\n    cached_data, cache_age_hours = self._load_cached_data_with_age(symbol, lookback_days)\n    if cached_data is not None and cache_age_hours is not None:\n        max_age_hours = self.MAX_DATA_AGE_HOURS\n        if cache_age_hours <= max_age_hours:\n            logger.info('%s: Using cached Polygon data (%.1f hours old) after all retries failed', symbol, cache_age_hours)\n            result.add_attempt(FetchAttempt(source=DataSource.CACHE, timestamp=time.time(), success=True, rows_fetched=len(cached_data), latency_ms=0.0))\n            return cached_data\n    return None"
    },
    {
      "name": "_fetch_polygon",
      "line": 1011,
      "code": "def _fetch_polygon(self, symbol: str, lookback_days: int) -> Optional[pd.DataFrame]:\n    \"\"\"Fetch market data from Polygon.io API.\"\"\"\n    if not self.polygon_api_key:\n        return None\n    try:\n        end_date = datetime.now().date()\n        start_date = end_date - timedelta(days=lookback_days + self.POLYGON_LOOKBACK_BUFFER_DAYS)\n        start_str = start_date.strftime('%Y-%m-%d')\n        end_str = end_date.strftime('%Y-%m-%d')\n        url = f'https://api.polygon.io/v2/aggs/ticker/{symbol.upper()}/range/1/day/{start_str}/{end_str}'\n        params = {'adjusted': 'true', 'sort': 'asc', 'limit': 120, 'apiKey': self.polygon_api_key}\n        response = self.session.get(url, params=params, timeout=10)\n        response.raise_for_status()\n        payload = response.json()\n        status = payload.get('status')\n        if status not in ('OK', 'DELAYED'):\n            if 'error' in payload:\n                raise ValueError(f\"Polygon.io API error: {payload['error']}\")\n            elif 'results' not in payload:\n                logger.warning('%s: Polygon.io returned no results (status: %s)', symbol, status)\n                return None\n        results = payload.get('results', [])\n        if not results:\n            logger.warning('%s: Polygon.io returned no bars (count: %s)', symbol, payload.get('resultsCount', 0))\n            return None\n        records = []\n        timestamps = []\n        for bar in results:\n            dt = datetime.fromtimestamp(bar['t'] / 1000)\n            timestamps.append(dt)\n            records.append({'Open': float(bar['o']), 'High': float(bar['h']), 'Low': float(bar['l']), 'Close': float(bar['c']), 'Volume': float(bar.get('v', 0))})\n        if not records:\n            return None\n        df = pd.DataFrame(records, index=timestamps)\n        df.index.name = 'Date'\n        df = df.sort_index()\n        logger.info('%s: Successfully fetched %d bars from Polygon.io', symbol, len(df))\n        return df\n    except Exception as exc:\n        logger.warning('%s: Polygon.io fetch failed: %s', symbol, exc)\n        return None"
    },
    {
      "name": "_cache_polygon_response",
      "line": 1096,
      "code": "def _cache_polygon_response(self, symbol: str, data: pd.DataFrame) -> None:\n    \"\"\"Cache successful Polygon response to disk for reuse.\"\"\"\n    try:\n        cache_file = self.cache_dir / f'{symbol.upper()}_{datetime.now().date()}.csv'\n        data.to_csv(cache_file, index=True)\n        logger.debug('%s: Cached Polygon response to %s', symbol, cache_file)\n    except Exception as exc:\n        logger.debug('%s: Failed to cache Polygon response: %s', symbol, exc)"
    },
    {
      "name": "_fetch_alpha_vantage",
      "line": 1107,
      "code": "def _fetch_alpha_vantage(self, symbol: str, max_total_time: float=90.0, start_time: Optional[float]=None) -> Optional[pd.DataFrame]:\n    \"\"\"\n        Fetch from Alpha Vantage with FAIL-FAST timeout logic.\n\n        CRITICAL FIX: If rate-limited, we FAIL IMMEDIATELY instead of waiting 10+ minutes.\n        This prevents GitHub Actions workflow timeouts (20 minute limit).\n\n        Args:\n            symbol: Stock symbol to fetch\n            max_total_time: Maximum total time to spend (default 90s)\n            start_time: Start time for timeout calculation (defaults to now)\n        \"\"\"\n    if not self.alpha_vantage_key:\n        logger.warning('%s: Alpha Vantage fallback unavailable (missing API key).', symbol)\n        return None\n    if start_time is None:\n        start_time = time.time()\n    cache_file = self.cache_dir / f'{symbol.upper()}_{datetime.utcnow().date()}.csv'\n    if cache_file.exists():\n        age = time.time() - cache_file.stat().st_mtime\n        if age <= self.CACHE_TTL_SECONDS:\n            try:\n                cached_df = pd.read_csv(cache_file, parse_dates=['Date'], index_col='Date')\n                if not cached_df.empty:\n                    logger.debug('%s: Using cached Alpha Vantage data (%.1f hours old)', symbol, age / 3600)\n                    return cached_df\n            except Exception as exc:\n                logger.debug('%s: Failed to load cached Alpha Vantage data: %s', symbol, exc)\n\n    def respect_rate_limit(min_interval: float) -> None:\n        elapsed = time.time() - self._last_alpha_call_ts\n        if elapsed < min_interval:\n            sleep_time = min_interval - elapsed\n            elapsed_total = time.time() - start_time\n            if elapsed_total + sleep_time > max_total_time:\n                raise TimeoutError(f'Would exceed max_total_time ({max_total_time}s) waiting for rate limit')\n            logger.debug('Sleeping %.2fs to respect Alpha Vantage rate limit', sleep_time)\n            time.sleep(sleep_time)\n    params = {'function': 'TIME_SERIES_DAILY_ADJUSTED', 'symbol': symbol, 'outputsize': 'compact', 'datatype': 'json', 'apikey': self.alpha_vantage_key}\n    for attempt in range(1, self.ALPHAVANTAGE_MAX_RETRIES + 1):\n        elapsed_total = time.time() - start_time\n        if elapsed_total >= max_total_time:\n            raise TimeoutError(f'Exceeded max_total_time ({max_total_time}s) after {attempt - 1} attempts')\n        respect_rate_limit(self.ALPHAVANTAGE_MIN_INTERVAL_SECONDS)\n        try:\n            response = self.session.get('https://www.alphavantage.co/query', params=params, timeout=30)\n            self._last_alpha_call_ts = time.time()\n            response.raise_for_status()\n            payload = response.json()\n        except Exception as exc:\n            logger.warning('Alpha Vantage request failed for %s (attempt %s): %s', symbol, attempt, exc)\n            continue\n        time_series = payload.get('Time Series (Daily)')\n        if time_series:\n            records = []\n            for date_str, values in time_series.items():\n                try:\n                    records.append({'Date': datetime.strptime(date_str, '%Y-%m-%d'), 'Open': float(values['1. open']), 'High': float(values['2. high']), 'Low': float(values['3. low']), 'Close': float(values['4. close']), 'Volume': float(values['6. volume'])})\n                except Exception as exc:\n                    logger.debug('%s: Skipping Alpha Vantage row %s (%s)', symbol, date_str, exc)\n            if records:\n                df = pd.DataFrame(records).set_index('Date').sort_index()\n                try:\n                    df.to_csv(cache_file, index=True)\n                except Exception as exc:\n                    logger.debug('%s: Unable to cache Alpha Vantage data: %s', symbol, exc)\n                return df\n        info_message = payload.get('Information') or payload.get('Note')\n        if info_message:\n            elapsed_total = time.time() - start_time\n            if elapsed_total >= max_total_time * 0.8:\n                raise TimeoutError(f'Alpha Vantage rate-limited after {elapsed_total:.1f}s. Message: {info_message}. Using cached data instead.')\n            max_wait = min(30.0, max_total_time - elapsed_total - 5)\n            if max_wait > 0:\n                logger.warning('%s: Alpha Vantage rate limit hit (attempt %s). Waiting %ss (max %ss). Message: %s', symbol, attempt, max_wait, max_total_time, info_message)\n                time.sleep(max_wait)\n            else:\n                raise TimeoutError(f'Alpha Vantage rate-limited. No time remaining (used {elapsed_total:.1f}s of {max_total_time}s)')\n            continue\n        logger.warning('%s: Alpha Vantage response missing time series (keys: %s)', symbol, list(payload.keys()))\n        if attempt < self.ALPHAVANTAGE_MAX_RETRIES:\n            elapsed_total = time.time() - start_time\n            max_wait = min(10.0, max_total_time - elapsed_total - 5)\n            if max_wait > 0:\n                time.sleep(max_wait)\n            else:\n                raise TimeoutError(f'No time remaining for retry (used {elapsed_total:.1f}s)')\n    return None"
    },
    {
      "name": "_load_cached_data",
      "line": 1287,
      "code": "def _load_cached_data(self, symbol: str, lookback_days: int) -> Optional[pd.DataFrame]:\n    \"\"\"Load cached data from disk as last resort fallback (legacy method).\"\"\"\n    data, _ = self._load_cached_data_with_age(symbol, lookback_days)\n    return data"
    },
    {
      "name": "_load_cached_data_with_age",
      "line": 1294,
      "code": "def _load_cached_data_with_age(self, symbol: str, lookback_days: int) -> Tuple[Optional[pd.DataFrame], Optional[float]]:\n    \"\"\"Load cached data from disk with age information.\"\"\"\n    try:\n        cache_pattern = self.cache_dir / f'{symbol.upper()}_*.csv'\n        import glob\n        cache_files = glob.glob(str(cache_pattern))\n        if not cache_files:\n            return (None, None)\n        cache_files.sort(key=lambda f: Path(f).stat().st_mtime, reverse=True)\n        latest_cache = Path(cache_files[0])\n        age_hours = (time.time() - latest_cache.stat().st_mtime) / 3600\n        max_age_hours = self.CACHE_MAX_AGE_DAYS * 24\n        if age_hours > max_age_hours:\n            logger.debug('%s: Cached data too old (%.1f hours > %d hours)', symbol, age_hours, max_age_hours)\n            return (None, None)\n        cached_df = pd.read_csv(latest_cache, parse_dates=['Date'], index_col='Date')\n        if not cached_df.empty and len(cached_df) >= lookback_days * 0.5:\n            logger.info('%s: Loaded %d rows from cache (%.1f hours old)', symbol, len(cached_df), age_hours)\n            return (cached_df.tail(lookback_days), age_hours)\n    except Exception as exc:\n        logger.debug('%s: Failed to load cached data: %s', symbol, exc)\n    return (None, None)"
    },
    {
      "name": "_is_valid",
      "line": 1343,
      "code": "@staticmethod\ndef _is_valid(data: Optional[pd.DataFrame], lookback_days: int) -> bool:\n    if data is None or data.empty:\n        return False\n    return len(data.index.unique()) >= lookback_days"
    },
    {
      "name": "_prepare",
      "line": 1349,
      "code": "@staticmethod\ndef _prepare(data: pd.DataFrame, lookback_days: int) -> pd.DataFrame:\n    df = data.copy().rename(columns={'Adj Close': 'Adj Close', 'Open': 'Open', 'High': 'High', 'Low': 'Low', 'Close': 'Close', 'Volume': 'Volume'})\n    if not isinstance(df.index, pd.DatetimeIndex):\n        df.index = pd.to_datetime(df.index)\n    df = df[~df.index.duplicated(keep='last')].sort_index()\n    return df.tail(lookback_days)"
    },
    {
      "name": "respect_rate_limit",
      "line": 1154,
      "code": "def respect_rate_limit(min_interval: float) -> None:\n    elapsed = time.time() - self._last_alpha_call_ts\n    if elapsed < min_interval:\n        sleep_time = min_interval - elapsed\n        elapsed_total = time.time() - start_time\n        if elapsed_total + sleep_time > max_total_time:\n            raise TimeoutError(f'Would exceed max_total_time ({max_total_time}s) waiting for rate limit')\n        logger.debug('Sleeping %.2fs to respect Alpha Vantage rate limit', sleep_time)\n        time.sleep(sleep_time)"
    }
  ]
}
