{
  "dead_imports": [
    {
      "line": 16,
      "module": "Tuple",
      "statement": "from typing import Dict, Any, Tuple"
    }
  ],
  "commented_blocks": [],
  "empty_functions": [],
  "unused_variables": [
    {
      "line": 161,
      "variable": "pl"
    }
  ],
  "all_functions": [
    {
      "name": "__init__",
      "line": 29,
      "code": "def __init__(self, learning_rate: float=0.1, discount_factor: float=0.95, exploration_rate: float=0.2, state_file: str='data/rl_policy_state.json'):\n    self.learning_rate = learning_rate\n    self.discount_factor = discount_factor\n    self.exploration_rate = exploration_rate\n    self.state_file = Path(state_file)\n    self.q_table: Dict[str, Dict[str, float]] = defaultdict(lambda: {'BUY': 0.0, 'SELL': 0.0, 'HOLD': 0.0})\n    self._load_state()\n    logger.info(f'RL Policy Learner initialized (\u03b1={learning_rate}, \u03b3={discount_factor}, \u03b5={exploration_rate})')"
    },
    {
      "name": "get_state_key",
      "line": 53,
      "code": "def get_state_key(self, market_state: Dict[str, Any]) -> str:\n    \"\"\"\n        Convert market state to discrete state key for Q-table.\n\n        Args:\n            market_state: Dict with market indicators\n\n        Returns:\n            State key string\n        \"\"\"\n    regime = market_state.get('market_regime', 'UNKNOWN')\n    rsi = market_state.get('rsi', 50)\n    rsi_bin = 'LOW' if rsi < 30 else 'HIGH' if rsi > 70 else 'MID'\n    macd_hist = market_state.get('macd_histogram', 0)\n    macd_bin = 'BULL' if macd_hist > 0 else 'BEAR'\n    trend = market_state.get('trend', 'SIDEWAYS')\n    return f'{regime}_{rsi_bin}_{macd_bin}_{trend}'"
    },
    {
      "name": "select_action",
      "line": 79,
      "code": "def select_action(self, market_state: Dict[str, Any], agent_recommendation: str) -> str:\n    \"\"\"\n        Select action using epsilon-greedy policy.\n\n        Args:\n            market_state: Current market state\n            agent_recommendation: Recommendation from agent consensus\n\n        Returns:\n            Selected action (BUY/SELL/HOLD)\n        \"\"\"\n    state_key = self.get_state_key(market_state)\n    if np.random.random() < self.exploration_rate:\n        action = agent_recommendation\n        logger.debug(f'RL: EXPLORE - using agent rec: {action}')\n    else:\n        q_values = self.q_table[state_key]\n        action = max(q_values, key=q_values.get)\n        logger.debug(f'RL: EXPLOIT - Q-values: {q_values}, selected: {action}')\n    return action"
    },
    {
      "name": "update_policy",
      "line": 107,
      "code": "def update_policy(self, prev_state: Dict[str, Any], action: str, reward: float, new_state: Dict[str, Any], done: bool=False) -> None:\n    \"\"\"\n        Update Q-values based on observed reward.\n\n        Args:\n            prev_state: Previous market state\n            action: Action taken\n            reward: Reward received (profit/loss)\n            new_state: New market state\n            done: Whether episode is done\n        \"\"\"\n    prev_key = self.get_state_key(prev_state)\n    new_key = self.get_state_key(new_state)\n    current_q = self.q_table[prev_key][action]\n    if done:\n        max_next_q = 0\n    else:\n        max_next_q = max(self.q_table[new_key].values())\n    new_q = current_q + self.learning_rate * (reward + self.discount_factor * max_next_q - current_q)\n    self.q_table[prev_key][action] = new_q\n    logger.info(f'RL UPDATE: State={prev_key}, Action={action}, Reward={reward:.4f}, Q: {current_q:.4f} -> {new_q:.4f}')\n    self._save_state()"
    },
    {
      "name": "calculate_reward",
      "line": 151,
      "code": "def calculate_reward(self, trade_result: Dict[str, Any]) -> float:\n    \"\"\"\n        Calculate reward from trade result.\n\n        Args:\n            trade_result: Dict with P/L, win/loss, etc.\n\n        Returns:\n            Reward value (normalized)\n        \"\"\"\n    pl = trade_result.get('pl', 0)\n    pl_pct = trade_result.get('pl_pct', 0)\n    reward = np.clip(pl_pct / 0.05, -1.0, 1.0)\n    return reward"
    },
    {
      "name": "_save_state",
      "line": 173,
      "code": "def _save_state(self) -> None:\n    \"\"\"Save Q-table to disk.\"\"\"\n    try:\n        self.state_file.parent.mkdir(parents=True, exist_ok=True)\n        q_table_dict = {k: dict(v) for k, v in self.q_table.items()}\n        state = {'q_table': q_table_dict, 'learning_rate': self.learning_rate, 'discount_factor': self.discount_factor, 'exploration_rate': self.exploration_rate}\n        with open(self.state_file, 'w') as f:\n            json.dump(state, f, indent=2)\n        logger.debug(f'RL state saved to {self.state_file}')\n    except Exception as e:\n        logger.error(f'Error saving RL state: {e}')"
    },
    {
      "name": "_load_state",
      "line": 196,
      "code": "def _load_state(self) -> None:\n    \"\"\"Load Q-table from disk.\"\"\"\n    if not self.state_file.exists():\n        logger.info('No existing RL state found - starting fresh')\n        return\n    try:\n        with open(self.state_file, 'r') as f:\n            state = json.load(f)\n        loaded_q_table = state.get('q_table', {})\n        for state_key, actions in loaded_q_table.items():\n            self.q_table[state_key] = actions\n        logger.info(f'RL state loaded: {len(self.q_table)} states learned')\n    except Exception as e:\n        logger.error(f'Error loading RL state: {e}')"
    },
    {
      "name": "get_policy_stats",
      "line": 216,
      "code": "def get_policy_stats(self) -> Dict[str, Any]:\n    \"\"\"Get statistics about learned policy.\"\"\"\n    total_states = len(self.q_table)\n    action_counts = {'BUY': 0, 'SELL': 0, 'HOLD': 0}\n    for state_actions in self.q_table.values():\n        best_action = max(state_actions, key=state_actions.get)\n        action_counts[best_action] += 1\n    return {'total_states_learned': total_states, 'action_distribution': action_counts, 'exploration_rate': self.exploration_rate}"
    }
  ]
}
