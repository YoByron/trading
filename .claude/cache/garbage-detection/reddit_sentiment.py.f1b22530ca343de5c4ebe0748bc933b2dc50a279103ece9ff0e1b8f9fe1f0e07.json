{
  "dead_imports": [
    {
      "line": 29,
      "module": "timedelta",
      "statement": "from datetime import datetime, timedelta"
    },
    {
      "line": 31,
      "module": "Counter",
      "statement": "from collections import defaultdict, Counter"
    }
  ],
  "commented_blocks": [],
  "empty_functions": [],
  "unused_variables": [
    {
      "line": 53,
      "variable": "SUBREDDITS"
    },
    {
      "line": 61,
      "variable": "BULLISH_KEYWORDS"
    },
    {
      "line": 89,
      "variable": "BEARISH_KEYWORDS"
    },
    {
      "line": 114,
      "variable": "TICKER_PATTERN"
    },
    {
      "line": 117,
      "variable": "EXCLUDED_TICKERS"
    }
  ],
  "all_functions": [
    {
      "name": "get_reddit_sentiment",
      "line": 595,
      "code": "def get_reddit_sentiment(subreddits: Optional[List[str]]=None, limit_per_sub: int=25, force_refresh: bool=False) -> Dict:\n    \"\"\"\n    Lightweight helper to fetch sentiment data with graceful degradation.\n    \"\"\"\n    try:\n        scraper = RedditSentiment()\n        return scraper.collect_daily_sentiment(subreddits=subreddits, limit_per_sub=limit_per_sub, force_refresh=force_refresh)\n    except Exception as exc:\n        logger.warning('Reddit sentiment unavailable: %s', exc)\n        return {'sentiment_by_ticker': {}, 'error': str(exc), 'subreddits': subreddits or RedditSentiment.SUBREDDITS}"
    },
    {
      "name": "main",
      "line": 619,
      "code": "def main():\n    \"\"\"CLI interface for Reddit sentiment scraping.\"\"\"\n    parser = argparse.ArgumentParser(description='Scrape Reddit sentiment for trading system')\n    parser.add_argument('--subreddits', type=str, default='wallstreetbets,stocks,investing,options', help='Comma-separated list of subreddits (default: wallstreetbets,stocks,investing,options)')\n    parser.add_argument('--limit', type=int, default=25, help='Posts to fetch per subreddit (default: 25)')\n    parser.add_argument('--force-refresh', action='store_true', help='Ignore cache and fetch fresh data')\n    parser.add_argument('--top', type=int, default=10, help='Show top N tickers (default: 10)')\n    parser.add_argument('--min-mentions', type=int, default=5, help='Minimum mentions to include in top tickers (default: 5)')\n    parser.add_argument('--output', type=str, help='Output file path (default: data/sentiment/reddit_YYYY-MM-DD.json)')\n    args = parser.parse_args()\n    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    scraper = RedditSentiment()\n    subreddits = [s.strip() for s in args.subreddits.split(',')]\n    logger.info('Starting Reddit sentiment collection...')\n    sentiment_data = scraper.collect_daily_sentiment(subreddits=subreddits, limit_per_sub=args.limit, force_refresh=args.force_refresh)\n    print('\\n' + '=' * 80)\n    print('REDDIT SENTIMENT ANALYSIS')\n    print('=' * 80)\n    print(f\"Date: {sentiment_data['meta']['date']}\")\n    print(f\"Subreddits: {', '.join(['r/' + s for s in subreddits])}\")\n    print(f\"Total Posts: {sentiment_data['meta']['total_posts']}\")\n    print(f\"Total Tickers: {sentiment_data['meta']['total_tickers']}\")\n    print(f'\\nTop {args.top} Tickers by Sentiment Score:')\n    print('-' * 80)\n    top_tickers = scraper.get_top_tickers(sentiment_data=sentiment_data, min_mentions=args.min_mentions, limit=args.top)\n    for i, (ticker, data) in enumerate(top_tickers, 1):\n        sentiment = 'BULLISH' if data['score'] > 0 else 'BEARISH' if data['score'] < 0 else 'NEUTRAL'\n        print(f\"{i}. {ticker:<6} | Score: {data['score']:>6} | Mentions: {data['mentions']:>3} | Confidence: {data['confidence'].upper():<6} | {sentiment}\")\n        print(f\"   Bullish Keywords: {data['bullish_keywords']}, Bearish Keywords: {data['bearish_keywords']}\")\n        print(f\"   Engagement: {data['total_upvotes']} upvotes, {data['total_comments']} comments\")\n        if data['top_posts']:\n            print(f\"   Top Post: {data['top_posts'][0]['title'][:60]}...\")\n        print()\n    print('\\nSubreddit Statistics:')\n    print('-' * 80)\n    for sub, stats in sentiment_data['meta']['subreddit_stats'].items():\n        status_icon = '\u2713' if stats['status'] == 'success' else '\u2717'\n        print(f\"{status_icon} r/{sub:<20} | Posts: {stats['posts_collected']:>3} | Status: {stats['status']}\")\n    print('\\n' + '=' * 80)\n    print(f\"Data saved to: {scraper.data_dir}/reddit_{sentiment_data['meta']['date']}.json\")\n    print('=' * 80)"
    },
    {
      "name": "__init__",
      "line": 164,
      "code": "def __init__(self, client_id: Optional[str]=None, client_secret: Optional[str]=None, user_agent: Optional[str]=None, data_dir: str='data/sentiment', cache_hours: int=24):\n    \"\"\"\n        Initialize Reddit sentiment scraper.\n\n        Args:\n            client_id: Reddit API client ID (optional - can use read-only mode)\n            client_secret: Reddit API client secret (optional)\n            user_agent: Reddit API user agent (optional)\n            data_dir: Directory to save sentiment data\n            cache_hours: Hours to cache results (default: 24)\n        \"\"\"\n    self.data_dir = Path(data_dir)\n    self.data_dir.mkdir(parents=True, exist_ok=True)\n    self.cache_hours = cache_hours\n    try:\n        if not client_id:\n            client_id = os.getenv('REDDIT_CLIENT_ID')\n        if not client_secret:\n            client_secret = os.getenv('REDDIT_CLIENT_SECRET')\n        if not user_agent:\n            user_agent = os.getenv('REDDIT_USER_AGENT', 'TradingBot/1.0 by AutomatedTrader')\n        if client_id and client_secret:\n            self.reddit = praw.Reddit(client_id=client_id, client_secret=client_secret, user_agent=user_agent)\n            logger.info('Initialized Reddit API with credentials')\n        else:\n            logger.warning('No Reddit API credentials provided!')\n            logger.warning('To use this scraper, create a Reddit app at:')\n            logger.warning('https://www.reddit.com/prefs/apps')\n            logger.warning('Then set REDDIT_CLIENT_ID and REDDIT_CLIENT_SECRET in .env')\n            raise ValueError('Reddit API credentials required. Set REDDIT_CLIENT_ID and REDDIT_CLIENT_SECRET in .env file. Create app at: https://www.reddit.com/prefs/apps')\n    except Exception as e:\n        logger.error(f'Failed to initialize Reddit API: {e}')\n        raise"
    },
    {
      "name": "scrape_subreddit",
      "line": 232,
      "code": "@retry_with_backoff(max_retries=3, initial_delay=2.0)\ndef scrape_subreddit(self, subreddit_name: str, limit: int=25, time_filter: str='day') -> List[Dict]:\n    \"\"\"\n        Scrape posts from a subreddit.\n\n        Args:\n            subreddit_name: Name of subreddit (without r/)\n            limit: Number of posts to fetch (default: 25)\n            time_filter: Time filter - 'day', 'week', 'month' (default: 'day')\n\n        Returns:\n            List of post dictionaries with metadata\n        \"\"\"\n    logger.info(f'Scraping r/{subreddit_name} (limit={limit}, filter={time_filter})')\n    try:\n        subreddit = self.reddit.subreddit(subreddit_name)\n        posts = []\n        for post in subreddit.hot(limit=limit):\n            post_age_hours = (datetime.utcnow() - datetime.utcfromtimestamp(post.created_utc)).total_seconds() / 3600\n            if time_filter == 'day' and post_age_hours > 24:\n                continue\n            elif time_filter == 'week' and post_age_hours > 168:\n                continue\n            posts.append({'id': post.id, 'title': post.title, 'text': post.selftext, 'author': str(post.author), 'created_utc': post.created_utc, 'score': post.score, 'upvote_ratio': post.upvote_ratio, 'num_comments': post.num_comments, 'flair': post.link_flair_text or '', 'url': post.url, 'permalink': f'https://reddit.com{post.permalink}'})\n        logger.info(f'Scraped {len(posts)} posts from r/{subreddit_name}')\n        return posts\n    except PRAWException as e:\n        logger.error(f'Error scraping r/{subreddit_name}: {e}')\n        raise"
    },
    {
      "name": "extract_tickers",
      "line": 289,
      "code": "def extract_tickers(self, text: str) -> List[str]:\n    \"\"\"\n        Extract stock tickers from text.\n\n        Args:\n            text: Text to search for tickers\n\n        Returns:\n            List of unique ticker symbols\n        \"\"\"\n    matches = self.TICKER_PATTERN.findall(text.upper())\n    tickers = [ticker for ticker in matches if ticker not in self.EXCLUDED_TICKERS and len(ticker) >= 1 and (len(ticker) <= 5)]\n    return list(set(tickers))"
    },
    {
      "name": "calculate_sentiment_score",
      "line": 314,
      "code": "def calculate_sentiment_score(self, text: str, upvotes: int=0, comments: int=0) -> Tuple[int, Dict[str, int]]:\n    \"\"\"\n        Calculate sentiment score for text.\n\n        Algorithm:\n        - Bullish keywords: +1 to +3 points\n        - Bearish keywords: -1 to -3 points\n        - Weight by upvotes (high upvotes = higher impact)\n        - Weight by comments (high engagement = more reliable)\n\n        Args:\n            text: Text to analyze\n            upvotes: Number of upvotes\n            comments: Number of comments\n\n        Returns:\n            Tuple of (total_score, keyword_counts)\n        \"\"\"\n    text_lower = text.lower()\n    bullish_count = 0\n    bearish_count = 0\n    keyword_details = {'bullish': 0, 'bearish': 0}\n    for keyword, weight in self.BULLISH_KEYWORDS.items():\n        if keyword in text_lower:\n            count = text_lower.count(keyword)\n            bullish_count += count * weight\n            keyword_details['bullish'] += count\n    for keyword, weight in self.BEARISH_KEYWORDS.items():\n        if keyword in text_lower:\n            count = text_lower.count(keyword)\n            bearish_count += count * abs(weight)\n            keyword_details['bearish'] += count\n    base_score = bullish_count + bearish_count\n    import math\n    upvote_weight = math.log(max(upvotes, 1) + 1)\n    comment_weight = math.log(max(comments, 1) + 1)\n    engagement_weight = (upvote_weight + comment_weight) / 2\n    weighted_score = int(base_score * engagement_weight)\n    return (weighted_score, keyword_details)"
    },
    {
      "name": "analyze_posts",
      "line": 372,
      "code": "def analyze_posts(self, posts: List[Dict]) -> Dict[str, Dict]:\n    \"\"\"\n        Analyze posts and aggregate sentiment by ticker.\n\n        Args:\n            posts: List of post dictionaries\n\n        Returns:\n            Dictionary mapping tickers to sentiment data\n        \"\"\"\n    ticker_data = defaultdict(lambda: {'mentions': 0, 'total_score': 0, 'bullish_keywords': 0, 'bearish_keywords': 0, 'total_upvotes': 0, 'total_comments': 0, 'posts': []})\n    for post in posts:\n        full_text = f\"{post['title']} {post['text']}\"\n        tickers = self.extract_tickers(full_text)\n        sentiment_score, keyword_counts = self.calculate_sentiment_score(full_text, upvotes=post['score'], comments=post['num_comments'])\n        for ticker in tickers:\n            ticker_data[ticker]['mentions'] += 1\n            ticker_data[ticker]['total_score'] += sentiment_score\n            ticker_data[ticker]['bullish_keywords'] += keyword_counts['bullish']\n            ticker_data[ticker]['bearish_keywords'] += keyword_counts['bearish']\n            ticker_data[ticker]['total_upvotes'] += post['score']\n            ticker_data[ticker]['total_comments'] += post['num_comments']\n            ticker_data[ticker]['posts'].append({'title': post['title'], 'score': post['score'], 'comments': post['num_comments'], 'flair': post['flair'], 'permalink': post['permalink'], 'sentiment_score': sentiment_score})\n    for ticker, data in ticker_data.items():\n        if data['mentions'] >= 10 and data['total_upvotes'] >= 100:\n            confidence = 'high'\n        elif data['mentions'] >= 5 and data['total_upvotes'] >= 50:\n            confidence = 'medium'\n        else:\n            confidence = 'low'\n        data['confidence'] = confidence\n    return dict(ticker_data)"
    },
    {
      "name": "collect_daily_sentiment",
      "line": 439,
      "code": "def collect_daily_sentiment(self, subreddits: Optional[List[str]]=None, limit_per_sub: int=25, force_refresh: bool=False) -> Dict:\n    \"\"\"\n        Collect daily sentiment from all subreddits.\n\n        Args:\n            subreddits: List of subreddit names (default: self.SUBREDDITS)\n            limit_per_sub: Posts to fetch per subreddit (default: 25)\n            force_refresh: Ignore cache and fetch fresh data (default: False)\n\n        Returns:\n            Dictionary with sentiment data\n        \"\"\"\n    today = datetime.now().strftime('%Y-%m-%d')\n    cache_file = self.data_dir / f'reddit_{today}.json'\n    if not force_refresh and cache_file.exists():\n        cache_age = datetime.now() - datetime.fromtimestamp(cache_file.stat().st_mtime)\n        if cache_age.total_seconds() < self.cache_hours * 3600:\n            logger.info(f'Loading cached sentiment data from {cache_file}')\n            with open(cache_file, 'r') as f:\n                return json.load(f)\n    subreddits = subreddits or self.SUBREDDITS\n    logger.info(f'Collecting sentiment from {len(subreddits)} subreddits')\n    all_posts = []\n    subreddit_stats = {}\n    for subreddit_name in subreddits:\n        try:\n            posts = self.scrape_subreddit(subreddit_name, limit=limit_per_sub, time_filter='day')\n            all_posts.extend(posts)\n            subreddit_stats[subreddit_name] = {'posts_collected': len(posts), 'status': 'success'}\n        except Exception as e:\n            logger.error(f'Failed to scrape r/{subreddit_name}: {e}')\n            subreddit_stats[subreddit_name] = {'posts_collected': 0, 'status': 'failed', 'error': str(e)}\n    ticker_sentiment = self.analyze_posts(all_posts)\n    sorted_tickers = sorted(ticker_sentiment.items(), key=lambda x: x[1]['total_score'], reverse=True)\n    output = {'meta': {'date': today, 'timestamp': datetime.now().isoformat(), 'subreddits': subreddits, 'total_posts': len(all_posts), 'total_tickers': len(ticker_sentiment), 'subreddit_stats': subreddit_stats}, 'sentiment_by_ticker': {ticker: {'score': data['total_score'], 'mentions': data['mentions'], 'confidence': data['confidence'], 'bullish_keywords': data['bullish_keywords'], 'bearish_keywords': data['bearish_keywords'], 'total_upvotes': data['total_upvotes'], 'total_comments': data['total_comments'], 'avg_score_per_mention': round(data['total_score'] / data['mentions'], 2) if data['mentions'] > 0 else 0, 'top_posts': sorted(data['posts'], key=lambda x: x['sentiment_score'], reverse=True)[:3]} for ticker, data in sorted_tickers}}\n    with open(cache_file, 'w') as f:\n        json.dump(output, f, indent=2)\n    logger.info(f'Saved sentiment data to {cache_file}')\n    logger.info(f'Analyzed {len(all_posts)} posts, found {len(ticker_sentiment)} tickers')\n    return output"
    },
    {
      "name": "get_top_tickers",
      "line": 547,
      "code": "def get_top_tickers(self, sentiment_data: Optional[Dict]=None, min_mentions: int=5, min_confidence: str='medium', limit: int=10) -> List[Tuple[str, Dict]]:\n    \"\"\"\n        Get top tickers by sentiment score.\n\n        Args:\n            sentiment_data: Sentiment data (default: load from today's cache)\n            min_mentions: Minimum mentions to include (default: 5)\n            min_confidence: Minimum confidence level (default: 'medium')\n            limit: Maximum tickers to return (default: 10)\n\n        Returns:\n            List of (ticker, data) tuples sorted by score\n        \"\"\"\n    if sentiment_data is None:\n        today = datetime.now().strftime('%Y-%m-%d')\n        cache_file = self.data_dir / f'reddit_{today}.json'\n        if cache_file.exists():\n            with open(cache_file, 'r') as f:\n                sentiment_data = json.load(f)\n        else:\n            logger.warning('No cached sentiment data found')\n            return []\n    confidence_levels = {'low': 0, 'medium': 1, 'high': 2}\n    min_conf_level = confidence_levels.get(min_confidence, 1)\n    filtered = [(ticker, data) for ticker, data in sentiment_data['sentiment_by_ticker'].items() if data['mentions'] >= min_mentions and confidence_levels.get(data['confidence'], 0) >= min_conf_level]\n    sorted_tickers = sorted(filtered, key=lambda x: x[1]['score'], reverse=True)\n    return sorted_tickers[:limit]"
    }
  ]
}
