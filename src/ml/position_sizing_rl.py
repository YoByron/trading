"""
Position Sizing RL Agent
RL agent that learns both action (BUY/SELL/HOLD) and optimal position size.
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Tuple, Optional
import logging

from src.ml.networks import LSTMPPO

logger = logging.getLogger(__name__)


class PositionSizingLSTMPPO(nn.Module):
    """
    LSTM-PPO Network with Position Sizing.

    Outputs:
    - Action probability (BUY/SELL/HOLD)
    - Optimal position size (0-1, representing 0-10% of portfolio)

    Architecture:
    - LSTM encoder for time-series features
    - Actor head: [action_probs, position_size]
    - Critic head: State value estimate
    """

    def __init__(
        self,
        input_dim: int,
        hidden_dim: int = 128,
        num_layers: int = 2,
        action_dim: int = 3,
        max_position_pct: float = 0.10  # Max 10% per trade
    ):
        super(PositionSizingLSTMPPO, self).__init__()

        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.max_position_pct = max_position_pct

        # LSTM Encoder
        self.lstm = nn.LSTM(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=0.2 if num_layers > 1 else 0
        )

        # Actor Head: Action probabilities + Position size
        self.actor_action = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.ReLU(),
            nn.Linear(64, action_dim),
            nn.Softmax(dim=-1)
        )

        # Position size head (sigmoid outputs 0-1, scaled to max_position_pct)
        self.actor_position_size = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()  # Outputs 0-1
        )

        # Critic Head (Value)
        self.critic = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )

    def forward(self, x: torch.Tensor, hidden: Optional[Tuple[torch.Tensor, torch.Tensor]] = None):
        """
        Forward pass.

        Args:
            x: Input tensor (batch, seq_len, input_dim)
            hidden: Optional LSTM hidden state

        Returns:
            action_probs: Action probabilities (batch, action_dim)
            position_size: Position size (batch, 1) in [0, max_position_pct]
            state_value: State value estimate (batch, 1)
            next_hidden: Updated hidden state
        """
        # LSTM encoding
        lstm_out, next_hidden = self.lstm(x, hidden)

        # Last time step
        last_step_out = lstm_out[:, -1, :]

        # Actor: Action probabilities
        action_probs = self.actor_action(last_step_out)

        # Actor: Position size (0-1, scaled to max_position_pct)
        position_size_raw = self.actor_position_size(last_step_out)
        position_size = position_size_raw * self.max_position_pct

        # Critic: State Value
        state_value = self.critic(last_step_out)

        return action_probs, position_size, state_value, next_hidden

    def get_action(
        self,
        x: torch.Tensor,
        hidden: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        deterministic: bool = False
    ) -> Tuple[int, float, float, Tuple[torch.Tensor, torch.Tensor]]:
        """
        Select action and position size.

        Args:
            x: Input state
            hidden: LSTM hidden state
            deterministic: If True, use argmax. If False, sample.

        Returns:
            action: Action index (0=Hold, 1=Buy, 2=Sell)
            position_size: Position size as percentage (0-10%)
            value: State value estimate
            hidden: Updated hidden state
        """
        action_probs, position_size, value, next_hidden = self.forward(x, hidden)

        # Select action
        if deterministic:
            action = torch.argmax(action_probs, dim=-1)
        else:
            dist = torch.distributions.Categorical(action_probs)
            action = dist.sample()

        # Get position size (convert to float)
        position_size_pct = float(position_size.item())

        return action.item(), position_size_pct, float(value.item()), next_hidden


class PositionSizingRLAgent:
    """
    RL Agent that learns optimal position sizes.

    This agent outputs both:
    1. Action (BUY/SELL/HOLD)
    2. Optimal position size (as percentage of portfolio)

    Expected Benefits:
    - Better capital efficiency (+20-30%)
    - Improved risk-adjusted returns (+0.2-0.3 Sharpe)
    - Lower drawdowns (-1-2%)
    """

    def __init__(
        self,
        input_dim: int,
        hidden_dim: int = 128,
        num_layers: int = 2,
        device: str = "cpu",
        max_position_pct: float = 0.10
    ):
        self.device = torch.device(device)
        self.max_position_pct = max_position_pct

        self.model = PositionSizingLSTMPPO(
            input_dim=input_dim,
            hidden_dim=hidden_dim,
            num_layers=num_layers,
            max_position_pct=max_position_pct
        ).to(self.device)

        logger.info(f"âœ… Position Sizing RL Agent initialized (max position: {max_position_pct*100:.1f}%)")

    def predict(
        self,
        state: torch.Tensor,
        deterministic: bool = False
    ) -> Tuple[int, float, float, Dict[str, Any]]:
        """
        Get action and position size prediction.

        Args:
            state: Input state tensor (batch, seq_len, features)
            deterministic: If True, use argmax. If False, sample.

        Returns:
            action: Selected action (0=Hold, 1=Buy, 2=Sell)
            position_size_pct: Position size as percentage (0-10%)
            confidence: Model confidence
            details: Additional prediction details
        """
        action, position_size_pct, value, _ = self.model.get_action(
            state.to(self.device),
            deterministic=deterministic
        )

        # Get action probabilities for confidence
        self.model.eval()
        with torch.no_grad():
            action_probs, _, _, _ = self.model.forward(state.to(self.device))
            confidence = float(action_probs[0, action].item())

        return action, position_size_pct, confidence, {
            'action_probs': action_probs[0].cpu().numpy().tolist(),
            'state_value': float(value),
            'position_size_pct': position_size_pct
        }

    def calculate_position_size(
        self,
        portfolio_value: float,
        position_size_pct: float,
        price_per_share: Optional[float] = None
    ) -> float:
        """
        Calculate actual position size in dollars or shares.

        Args:
            portfolio_value: Total portfolio value
            position_size_pct: Position size percentage (from model)
            price_per_share: Price per share (if None, returns dollars)

        Returns:
            Position size in dollars or shares
        """
        position_value = portfolio_value * position_size_pct

        if price_per_share and price_per_share > 0:
            return int(position_value / price_per_share)  # Return shares

        return position_value  # Return dollars
