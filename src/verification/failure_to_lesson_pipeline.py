"""
Failure-to-Lesson-to-Test Pipeline.

Closes the loop between:
1. Trade/System Failures → Detected by AnomalyLearningLoop
2. Lessons Learned → Written as searchable markdown files
3. Regression Tests → Auto-generated by test_auto_learn_from_failures.py
4. Prevention → RAG queries before trades

This module adds the MISSING LINK: writing lessons as markdown files
that the test generator can parse.

Usage:
    from src.verification.failure_to_lesson_pipeline import FailureToLessonPipeline

    pipeline = FailureToLessonPipeline()

    # From anomaly detection
    lesson_path = pipeline.create_lesson_from_anomaly(anomaly_dict)

    # From trade failure
    lesson_path = pipeline.create_lesson_from_trade_failure(trade_result)

    # Trigger test regeneration
    pipeline.regenerate_tests()

Author: Trading System
Created: 2025-12-15
"""

import logging
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Optional

logger = logging.getLogger(__name__)

LESSONS_DIR = Path("rag_knowledge/lessons_learned")
ANOMALY_RECURRENCE_PATH = Path("data/anomaly_recurrence.json")


class FailureToLessonPipeline:
    """
    Complete pipeline from failures to prevention.

    Flow:
    1. Failure detected (anomaly, trade loss, test failure)
    2. Create structured markdown lesson
    3. Ingest into RAG for semantic search
    4. Auto-generate regression test
    5. Next similar issue is caught before it happens
    """

    SEVERITY_MAP = {
        "block": "CRITICAL",
        "critical": "CRITICAL",
        "warning": "HIGH",
        "high": "HIGH",
        "medium": "MEDIUM",
        "info": "LOW",
        "low": "LOW",
    }

    CATEGORY_MAP = {
        "order_amount": "Trading",
        "order_frequency": "Trading",
        "price_deviation": "Market",
        "data_staleness": "Data",
        "execution_failure": "Execution",
        "symbol_unknown": "Configuration",
        "market_hours": "Trading",
        "position_size": "Risk",
        "volatility_spike": "Market",
        "import_error": "Code",
        "syntax_error": "Code",
        "test_failure": "Testing",
        "api_error": "Integration",
    }

    def __init__(self, lessons_dir: Optional[Path] = None):
        self.lessons_dir = lessons_dir or LESSONS_DIR
        self.lessons_dir.mkdir(parents=True, exist_ok=True)

    def get_next_lesson_number(self) -> int:
        """Get the next available lesson number."""
        max_num = 0
        for file in self.lessons_dir.glob("ll_*.md"):
            try:
                # Extract number from filename like ll_035_topic_dec15.md
                num_str = file.stem.split("_")[1]
                num = int(num_str)
                if num > max_num:
                    max_num = num
            except (IndexError, ValueError):
                continue
        return max_num + 1

    def create_lesson_from_anomaly(
        self,
        anomaly: dict,
        additional_context: Optional[str] = None,
    ) -> Path:
        """
        Create a markdown lesson from an anomaly detection.

        Args:
            anomaly: Dict with keys: anomaly_id, type, level, message, details, context
            additional_context: Extra context to include

        Returns:
            Path to created lesson file
        """
        lesson_num = self.get_next_lesson_number()
        today = datetime.now()
        date_str = today.strftime("%Y-%m-%d")
        month_day = today.strftime("%b%d").lower()

        # Map anomaly type to category
        anomaly_type = anomaly.get("type", "unknown")
        category = self.CATEGORY_MAP.get(anomaly_type, "System")
        severity = self.SEVERITY_MAP.get(anomaly.get("level", "medium").lower(), "MEDIUM")

        # Extract file patterns from context
        file_patterns = self._extract_file_patterns(anomaly)
        import_checks = self._extract_import_checks(anomaly)

        # Generate title from message
        title = self._generate_title(anomaly)
        topic_slug = self._slugify(anomaly_type)

        # Build markdown content
        content = self._build_lesson_markdown(
            lesson_num=lesson_num,
            date_str=date_str,
            title=title,
            severity=severity,
            category=category,
            anomaly=anomaly,
            file_patterns=file_patterns,
            import_checks=import_checks,
            additional_context=additional_context,
        )

        # Write file
        filename = f"ll_{lesson_num:03d}_{topic_slug}_{month_day}.md"
        lesson_path = self.lessons_dir / filename

        lesson_path.write_text(content)
        logger.info(f"Created lesson: {lesson_path}")

        return lesson_path

    def create_lesson_from_trade_failure(
        self,
        trade_result: dict,
        loss_threshold: float = -50.0,
    ) -> Optional[Path]:
        """
        Create a lesson from a losing trade if significant.

        Args:
            trade_result: Dict with keys: symbol, pnl, entry_price, exit_price,
                         strategy, reason, timestamp
            loss_threshold: Only create lesson if loss exceeds this (negative value)

        Returns:
            Path to created lesson file, or None if loss too small
        """
        pnl = trade_result.get("pnl", 0)

        # Only create lessons for significant losses
        if pnl > loss_threshold:
            logger.debug(f"Trade loss {pnl} below threshold {loss_threshold}, skipping")
            return None

        # Convert to anomaly format
        anomaly = {
            "anomaly_id": f"TRADE-{datetime.now().strftime('%Y%m%d%H%M%S')}",
            "type": "trade_loss",
            "level": "high" if pnl < -100 else "medium",
            "message": f"Trade loss of ${abs(pnl):.2f} on {trade_result.get('symbol', 'UNKNOWN')}",
            "details": {
                "pnl": pnl,
                "symbol": trade_result.get("symbol"),
                "entry_price": trade_result.get("entry_price"),
                "exit_price": trade_result.get("exit_price"),
                "strategy": trade_result.get("strategy"),
                "reason": trade_result.get("reason"),
            },
            "detected_at": trade_result.get("timestamp", datetime.now().isoformat()),
            "context": {
                "action": "SELL" if pnl < 0 else "BUY",
                "symbol": trade_result.get("symbol"),
            },
        }

        return self.create_lesson_from_anomaly(
            anomaly,
            additional_context=f"Strategy: {trade_result.get('strategy', 'Unknown')}"
        )

    def create_lesson_from_test_failure(
        self,
        test_name: str,
        error_message: str,
        file_path: Optional[str] = None,
        traceback: Optional[str] = None,
    ) -> Path:
        """
        Create a lesson from a test failure.

        Args:
            test_name: Name of the failed test
            error_message: Error message
            file_path: Path to the failing test file
            traceback: Full traceback if available

        Returns:
            Path to created lesson file
        """
        anomaly = {
            "anomaly_id": f"TEST-{datetime.now().strftime('%Y%m%d%H%M%S')}",
            "type": "test_failure",
            "level": "high",
            "message": f"Test failure: {test_name}",
            "details": {
                "test_name": test_name,
                "error": error_message,
                "file": file_path,
            },
            "detected_at": datetime.now().isoformat(),
            "context": {
                "file": file_path,
                "traceback": traceback[:500] if traceback else None,
            },
        }

        return self.create_lesson_from_anomaly(anomaly)

    def regenerate_tests(self) -> bool:
        """
        Trigger test regeneration from lessons.

        Returns:
            True if tests were regenerated successfully
        """
        try:
            # Run pytest collection to regenerate dynamic tests
            result = subprocess.run(
                ["python", "-m", "pytest", "--collect-only", "-q",
                 "tests/test_auto_learn_from_failures.py"],
                capture_output=True,
                text=True,
                timeout=30,
            )

            if result.returncode == 0:
                logger.info("Tests regenerated successfully")
                return True
            else:
                logger.warning(f"Test regeneration had issues: {result.stderr}")
                return False

        except Exception as e:
            logger.error(f"Failed to regenerate tests: {e}")
            return False

    def _generate_title(self, anomaly: dict) -> str:
        """Generate a descriptive title from anomaly."""
        message = anomaly.get("message", "")
        anomaly_type = anomaly.get("type", "Unknown")

        if message:
            # Clean up and truncate
            title = message.split("\n")[0][:80]
            return title

        return f"{anomaly_type.replace('_', ' ').title()} Issue"

    def _slugify(self, text: str) -> str:
        """Convert text to URL-safe slug."""
        return text.lower().replace(" ", "_").replace("-", "_")[:30]

    def _extract_file_patterns(self, anomaly: dict) -> list[str]:
        """Extract file patterns from anomaly for test generation."""
        patterns = []

        # Check context for file references
        context = anomaly.get("context", {})
        details = anomaly.get("details", {})

        for data in [context, details]:
            for key, value in data.items():
                if isinstance(value, str):
                    if value.endswith(".py") or "/src/" in value:
                        patterns.append(value)

        return patterns

    def _extract_import_checks(self, anomaly: dict) -> list[str]:
        """Extract import statements to verify."""
        imports = []

        details = anomaly.get("details", {})
        if "module" in details:
            imports.append(f"import {details['module']}")

        # Common imports to check based on type
        type_imports = {
            "execution_failure": ["from src.orchestrator.main import TradingOrchestrator"],
            "position_size": ["from src.risk.position_sizer import PositionSizer"],
            "data_staleness": ["from src.rag.vector_db.chroma_client import ChromaClient"],
        }

        anomaly_type = anomaly.get("type", "")
        imports.extend(type_imports.get(anomaly_type, []))

        return imports

    def _build_lesson_markdown(
        self,
        lesson_num: int,
        date_str: str,
        title: str,
        severity: str,
        category: str,
        anomaly: dict,
        file_patterns: list[str],
        import_checks: list[str],
        additional_context: Optional[str] = None,
    ) -> str:
        """Build the markdown content for a lesson."""
        details = anomaly.get("details", {})
        context = anomaly.get("context", {})
        anomaly_type = anomaly.get("type", "unknown")

        # Build tags
        tags = ["auto-generated", anomaly_type, category.lower()]
        if anomaly.get("context", {}).get("symbol"):
            tags.append(anomaly["context"]["symbol"])

        tags_str = ", ".join(tags)

        # Format details
        details_str = "\n".join(f"- {k}: {v}" for k, v in details.items() if v)
        context_str = "\n".join(f"- {k}: {v}" for k, v in context.items() if v)

        # Generate prevention based on type
        prevention = self._generate_prevention(anomaly_type, details)

        # Build file patterns section
        file_patterns_section = ""
        if file_patterns:
            patterns_list = "\n".join(f"- `{p}`" for p in file_patterns)
            file_patterns_section = f"""
## Files to Check

{patterns_list}
"""

        # Build import checks section
        import_section = ""
        if import_checks:
            imports_code = "\n".join(import_checks)
            import_section = f"""
## Import Verification

```python
{imports_code}
```
"""

        content = f"""# Lesson Learned: {title}

**ID**: LL_{lesson_num:03d}
**Date**: {date_str}
**Severity**: {severity}
**Category**: {category}
**Tags**: {tags_str}

## Incident Summary

{anomaly.get("message", "No message provided")}

## Details

{details_str if details_str else "No additional details"}

## Context

{context_str if context_str else "No context available"}
{additional_context or ""}

## Root Cause

{self._infer_root_cause(anomaly_type, details)}

## Prevention Measures

{prevention}
{file_patterns_section}{import_section}
## Detection Method

Auto-generated by FailureToLessonPipeline from {anomaly_type} detection.

**Anomaly ID**: {anomaly.get("anomaly_id", "N/A")}
**Detected At**: {anomaly.get("detected_at", "Unknown")}
"""
        return content

    def _generate_prevention(self, anomaly_type: str, details: dict) -> str:
        """Generate prevention measures based on anomaly type."""
        preventions = {
            "order_amount": """1. Add pre-trade validation: `assert order_amount <= max_daily_amount * 2`
2. Implement order amount circuit breaker
3. Log and alert on amounts > threshold""",

            "trade_loss": f"""1. Review strategy parameters for {details.get('symbol', 'this symbol')}
2. Add stop-loss if not present: recommended 2% max loss
3. Backtest strategy with recent data
4. Consider reducing position size""",

            "data_staleness": """1. Validate data timestamp < 5 minutes before trading
2. Implement data freshness check in pre-trade validation
3. Add monitoring alert for stale data""",

            "test_failure": """1. Fix the failing test
2. Add related edge case tests
3. Update documentation if behavior changed""",

            "execution_failure": """1. Implement retry logic with exponential backoff
2. Add fallback execution path
3. Monitor API health before trading""",

            "position_size": """1. Enforce position size limits at execution layer
2. Add position size validation in pre-trade checks
3. Log position size calculations for audit""",
        }

        return preventions.get(anomaly_type, f"""1. Add validation for {anomaly_type}
2. Implement monitoring and alerting
3. Review related code paths for similar issues""")

    def _infer_root_cause(self, anomaly_type: str, details: dict) -> str:
        """Infer root cause from anomaly type and details."""
        root_causes = {
            "order_amount": "Order amount calculation error or unit conversion mistake",
            "trade_loss": "Strategy underperformance or adverse market conditions",
            "data_staleness": "Data pipeline delay or API failure",
            "test_failure": "Code change broke existing functionality or test assumption",
            "execution_failure": "API error, insufficient funds, or market conditions",
            "position_size": "Position sizing calculation error or risk limit violation",
        }

        base_cause = root_causes.get(anomaly_type, f"Unknown root cause for {anomaly_type}")

        # Add specific details
        if "multiplier" in details:
            base_cause += f" (Amount was {details['multiplier']:.1f}x expected)"
        if "pnl" in details:
            base_cause += f" (P/L: ${details['pnl']:.2f})"

        return base_cause


# =============================================================================
# INTEGRATION WITH EXISTING SYSTEMS
# =============================================================================


def integrate_with_anomaly_loop():
    """
    Patch AnomalyLearningLoop to also write markdown files.

    Call this at startup to enable full pipeline.
    """
    from src.verification.anomaly_learning_feedback_loop import AnomalyLearningLoop

    pipeline = FailureToLessonPipeline()
    original_auto_ingest = AnomalyLearningLoop.auto_ingest_lesson

    def enhanced_auto_ingest(self, anomaly: dict) -> str:
        # Call original RAG ingestion
        lesson_id = original_auto_ingest(self, anomaly)

        # Also write markdown file for test generation
        try:
            pipeline.create_lesson_from_anomaly(anomaly)
            logger.info(f"Created markdown lesson for {anomaly['type']}")
        except Exception as e:
            logger.warning(f"Failed to create markdown lesson: {e}")

        return lesson_id

    AnomalyLearningLoop.auto_ingest_lesson = enhanced_auto_ingest
    logger.info("Integrated FailureToLessonPipeline with AnomalyLearningLoop")


if __name__ == "__main__":
    """Demo the failure-to-lesson pipeline."""
    logging.basicConfig(level=logging.INFO)

    print("=" * 80)
    print("FAILURE-TO-LESSON PIPELINE DEMO")
    print("=" * 80)

    pipeline = FailureToLessonPipeline()

    # Demo 1: Create lesson from anomaly
    print("\n1. Creating lesson from anomaly...")
    anomaly = {
        "anomaly_id": "DEMO-001",
        "type": "order_amount",
        "level": "warning",
        "message": "Order amount $150 exceeds threshold $100",
        "details": {
            "amount": 150,
            "threshold": 100,
            "multiplier": 1.5,
        },
        "detected_at": datetime.now().isoformat(),
        "context": {
            "symbol": "SPY",
            "strategy": "momentum",
        },
    }

    lesson_path = pipeline.create_lesson_from_anomaly(anomaly)
    print(f"   Created: {lesson_path}")

    # Demo 2: Create lesson from trade loss
    print("\n2. Creating lesson from trade loss...")
    trade = {
        "symbol": "NVDA",
        "pnl": -75.50,
        "entry_price": 142.00,
        "exit_price": 138.50,
        "strategy": "mean_reversion",
        "reason": "Stop loss triggered",
        "timestamp": datetime.now().isoformat(),
    }

    lesson_path = pipeline.create_lesson_from_trade_failure(trade)
    if lesson_path:
        print(f"   Created: {lesson_path}")
    else:
        print("   Skipped (loss below threshold)")

    print("\n" + "=" * 80)
    print("Pipeline ready. Run pytest to verify test generation:")
    print("  pytest tests/test_auto_learn_from_failures.py -v")
    print("=" * 80)
