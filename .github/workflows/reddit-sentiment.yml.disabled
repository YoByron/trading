name: Daily Reddit Sentiment Collection

on:
  schedule:
    # Run at 9:00 AM ET (14:00 UTC) before market opens at 9:30 AM
    - cron: '0 14 * * 1-5'  # Weekdays only
  workflow_dispatch:  # Manual trigger

permissions:
  contents: write

jobs:
  collect-sentiment:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install requests praw

      - name: Collect Reddit Sentiment
        env:
          REDDIT_CLIENT_ID: ${{ secrets.REDDIT_CLIENT_ID }}
          REDDIT_CLIENT_SECRET: ${{ secrets.REDDIT_CLIENT_SECRET }}
          REDDIT_USER_AGENT: "TradingBot/1.0"
        run: |
          python3 << 'PYTHON'
          import json
          import os
          import re
          import time
          from datetime import datetime
          from pathlib import Path
          import requests

          # Subreddits to monitor (matches src/utils/reddit_sentiment.py)
          SUBREDDITS = [
              "wallstreetbets",
              "stocks",
              "investing",
              "options",
              "thetagang",
          ]

          # Bullish keywords
          BULLISH = {
              "moon": 3, "rocket": 3, "calls": 2, "buy": 2, "long": 2,
              "bullish": 2, "gains": 2, "profit": 2, "bull": 2, "rally": 2,
              "wheel": 2, "csp": 2, "theta decay": 2, "iv crush": 2,
              "put credit spread": 2, "credit spread": 1,
          }

          # Bearish keywords
          BEARISH = {
              "dump": -3, "crash": -3, "puts": -2, "sell": -2, "short": -2,
              "bearish": -2, "loss": -2, "bear": -2, "tank": -3, "collapse": -3,
              "max loss": -3, "blown out": -3, "call credit spread": -2,
          }

          def fetch_subreddit(name: str, limit: int = 25):
              """Fetch posts from subreddit using public JSON API."""
              url = f"https://www.reddit.com/r/{name}/hot.json?limit={limit}"
              headers = {"User-Agent": "TradingBot/1.0"}
              try:
                  resp = requests.get(url, headers=headers, timeout=10)
                  if resp.status_code == 200:
                      return resp.json().get("data", {}).get("children", [])
              except Exception as e:
                  print(f"Error fetching r/{name}: {e}")
              return []

          def analyze_sentiment(text: str) -> tuple:
              """Analyze text for sentiment and ticker mentions."""
              text_lower = text.lower()
              score = 0

              for word, weight in BULLISH.items():
                  if word in text_lower:
                      score += weight * text_lower.count(word)

              for word, weight in BEARISH.items():
                  if word in text_lower:
                      score += weight * text_lower.count(word)

              # Extract tickers
              tickers = set(re.findall(r'\$?([A-Z]{2,5})\b', text))
              # Filter common words
              exclude = {"THE", "AND", "FOR", "CEO", "CFO", "IPO", "ETF", "USD", "EUR"}
              tickers = [t for t in tickers if t not in exclude]

              return score, tickers

          # Collect data
          results = {
              "date": datetime.now().strftime("%Y-%m-%d"),
              "timestamp": datetime.now().isoformat(),
              "subreddits": {},
              "ticker_sentiment": {},
              "overall_sentiment": 0,
          }

          total_score = 0
          post_count = 0

          for sub in SUBREDDITS:
              print(f"Fetching r/{sub}...")
              posts = fetch_subreddit(sub)
              sub_data = {"posts": [], "sentiment": 0}

              for post in posts:
                  data = post.get("data", {})
                  title = data.get("title", "")
                  selftext = data.get("selftext", "")
                  text = f"{title} {selftext}"

                  score, tickers = analyze_sentiment(text)

                  sub_data["posts"].append({
                      "title": title[:100],
                      "score": score,
                      "tickers": tickers,
                      "upvotes": data.get("ups", 0),
                  })
                  sub_data["sentiment"] += score
                  total_score += score
                  post_count += 1

                  # Track per-ticker sentiment
                  for ticker in tickers:
                      if ticker not in results["ticker_sentiment"]:
                          results["ticker_sentiment"][ticker] = {"score": 0, "mentions": 0}
                      results["ticker_sentiment"][ticker]["score"] += score
                      results["ticker_sentiment"][ticker]["mentions"] += 1

              results["subreddits"][sub] = sub_data
              time.sleep(2)  # Rate limiting

          results["overall_sentiment"] = total_score
          results["post_count"] = post_count

          # Save to data directory
          data_dir = Path("data/sentiment")
          data_dir.mkdir(parents=True, exist_ok=True)

          date_str = datetime.now().strftime("%Y-%m-%d")
          output_file = data_dir / f"reddit_{date_str}.json"
          output_file.write_text(json.dumps(results, indent=2))

          print(f"\n=== Reddit Sentiment Summary ===")
          print(f"Date: {date_str}")
          print(f"Posts analyzed: {post_count}")
          print(f"Overall sentiment: {total_score}")
          print(f"Saved to: {output_file}")

          # Show top tickers
          top_tickers = sorted(
              results["ticker_sentiment"].items(),
              key=lambda x: abs(x[1]["score"]),
              reverse=True
          )[:10]

          print(f"\nTop tickers by sentiment:")
          for ticker, data in top_tickers:
              print(f"  {ticker}: {data['score']:+d} ({data['mentions']} mentions)")
          PYTHON

      - name: Save to RAG knowledge
        run: |
          DATE=$(date +%Y-%m-%d)
          DAY=$(date +%d)

          # Create RAG document from sentiment
          cat > "rag_knowledge/market_intel/reddit_sentiment_dec${DAY}.md" << EOF
          # Reddit Sentiment Report - $(date +"%B %d, %Y")

          ## Summary
          Daily sentiment collected from r/wallstreetbets, r/stocks, r/investing, r/options, r/thetagang.

          ## Key Signals
          $(cat data/sentiment/reddit_${DATE}.json | python3 -c "
          import json, sys
          data = json.load(sys.stdin)
          print(f\"- Overall sentiment: {data['overall_sentiment']:+d}\")
          print(f\"- Posts analyzed: {data['post_count']}\")
          print()
          print('### Top Bullish Tickers')
          tickers = sorted(data['ticker_sentiment'].items(), key=lambda x: x[1]['score'], reverse=True)[:5]
          for t, d in tickers:
              if d['score'] > 0:
                  print(f\"- {t}: {d['score']:+d} ({d['mentions']} mentions)\")
          print()
          print('### Top Bearish Tickers')
          tickers = sorted(data['ticker_sentiment'].items(), key=lambda x: x[1]['score'])[:5]
          for t, d in tickers:
              if d['score'] < 0:
                  print(f\"- {t}: {d['score']:+d} ({d['mentions']} mentions)\")
          ")

          ## Source
          Automated collection via GitHub Actions workflow.
          EOF

          echo "Created RAG document"

      - name: Commit and push
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add data/sentiment/ rag_knowledge/market_intel/
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git commit -m "data: Daily Reddit sentiment $(date +%Y-%m-%d)"
            git push origin main
          fi
