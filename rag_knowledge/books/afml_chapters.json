{
  "book_id": "afml",
  "title": "Advances in Financial Machine Learning",
  "author": "Marcos López de Prado",
  "year": 2018,
  "chapters": [
    {
      "chapter_num": 2,
      "title": "Financial Data Structures",
      "pages": "25-42",
      "topics": ["tick_bars", "volume_bars", "dollar_bars", "imbalance_bars"],
      "summary": "Standard time-sampled bars contain uneven information. Tick bars, volume bars, and dollar bars sample by market activity rather than time, producing more normally distributed returns and reducing heteroscedasticity. Information-driven bars (tick imbalance, volume imbalance) detect when informed traders are active.",
      "key_points": [
        "Time bars oversample low-activity periods and undersample high-activity periods, leading to poor statistical properties",
        "Volume bars sample when a fixed amount of volume has traded, normalizing return distributions",
        "Dollar bars are particularly useful for assets that have split or changed price significantly",
        "Tick imbalance bars detect when buying or selling pressure becomes significant - use θ = E[T] * (2P[b=1] - 1) as threshold",
        "Volume/dollar imbalance bars weight each tick by its size, providing more stable thresholds"
      ],
      "formulas": [
        {
          "name": "Tick Imbalance Bar Threshold",
          "expression": "θ = E[T] * |2P[b=1] - 1|",
          "explanation": "Expected tick imbalance where T is expected bar length and P[b=1] is probability of uptick. A new bar forms when cumulative signed ticks exceed θ."
        },
        {
          "name": "Expected Bar Size",
          "expression": "E[T] = E[T_0] * |2P[b=1] - 1| / |θ_t - θ_{t-1}|",
          "explanation": "Dynamically adjusts expected bar length based on market activity changes."
        }
      ]
    },
    {
      "chapter_num": 3,
      "title": "Labeling",
      "pages": "43-60",
      "topics": ["triple_barrier", "meta_labeling", "fixed_time_horizon", "labels"],
      "summary": "The triple-barrier method labels observations based on which of three barriers is touched first: upper horizontal (profit-take), lower horizontal (stop-loss), or vertical (max holding period). Meta-labeling uses a secondary model to size positions from a primary model's signals, dramatically improving Sharpe ratios.",
      "key_points": [
        "Fixed-time horizon labeling ignores path dependency - a trade that hits stop-loss then recovers is mislabeled as win",
        "Triple-barrier method uses: (1) upper profit barrier, (2) lower stop-loss barrier, (3) vertical time barrier",
        "When vertical barrier is hit first, label is determined by return sign at that point",
        "Meta-labeling: First model generates signals (high recall, low precision acceptable), second model filters",
        "Meta-labeling model outputs position size (0 to 1), not direction - direction comes from primary model",
        "CRITICAL: Meta-labeling allows you to use ML models with low precision as long as recall is high"
      ],
      "formulas": [
        {
          "name": "Triple Barrier Label",
          "expression": "y = {1 if upper barrier hit first, -1 if lower barrier hit first, sign(r) if vertical hit first}",
          "explanation": "Where r is the return at the time of the vertical barrier. This captures the actual path-dependent outcome."
        },
        {
          "name": "Dynamic Barrier Width",
          "expression": "width = volatility_multiple * daily_volatility",
          "explanation": "Barriers should scale with volatility. Typical multiples: 1-2x for profit target, 1x for stop-loss."
        }
      ]
    },
    {
      "chapter_num": 4,
      "title": "Sample Weights",
      "pages": "61-78",
      "topics": ["sample_weights", "uniqueness", "sequential_bootstrap", "decay"],
      "summary": "Overlapping labels cause samples to be non-IID. Sequential bootstrap and sample uniqueness weights correct for this. Without proper weighting, backtests dramatically overfit because they reuse the same information multiple times.",
      "key_points": [
        "Labels that span overlapping periods share information - treating them as IID causes massive overfitting",
        "Average uniqueness measures what fraction of a label's information is not shared with concurrent labels",
        "Sample weights should be proportional to average uniqueness - more unique samples matter more",
        "Sequential bootstrap: Sample with replacement but weight by uniqueness probability",
        "Time decay: Recent samples should get higher weight - use c parameter (typical 0.5) for exponential decay",
        "CRITICAL: Without sample weights, your backtest's Sharpe is inflated by sqrt(number of overlapping labels)"
      ],
      "formulas": [
        {
          "name": "Average Uniqueness",
          "expression": "u_i = (1/|c_i|) * Σ_{t∈c_i} 1/count_t",
          "explanation": "Where c_i is the set of timestamps spanned by label i, and count_t is how many labels include timestamp t."
        },
        {
          "name": "Time Decay Weight",
          "expression": "w_i = c^(T-t_i)",
          "explanation": "Where c ∈ (0,1) is decay factor, T is latest timestamp, t_i is sample timestamp. c=0.5 means sample from T-1 has half the weight."
        }
      ]
    },
    {
      "chapter_num": 5,
      "title": "Fractionally Differentiated Features",
      "pages": "79-96",
      "topics": ["stationarity", "fractional_differentiation", "memory", "adf_test"],
      "summary": "Price series are non-stationary (unit root) which breaks ML assumptions. Standard differencing (returns) loses all memory. Fractional differentiation with d<1 achieves stationarity while preserving memory - the sweet spot is the minimum d that passes ADF test.",
      "key_points": [
        "Non-stationary series violate OLS assumptions, causing spurious regression and overfitting",
        "Returns (d=1 differencing) are stationary but lose all memory of price levels",
        "Fractional differentiation with d between 0.3-0.7 typically achieves stationarity while keeping memory",
        "Use ADF test to find minimum d that achieves stationarity (p-value < 0.05)",
        "Fixed-width window FFD is computationally practical: weights decay geometrically",
        "CRITICAL: Features with d≈0.4 often have better predictive power than returns"
      ],
      "formulas": [
        {
          "name": "Fractional Differentiation Weights",
          "expression": "w_k = -w_{k-1} * (d-k+1)/k",
          "explanation": "Recursive formula for weights. w_0 = 1. For d=0.5, weights decay roughly as 1/sqrt(k)."
        },
        {
          "name": "Fixed-Width Window FFD",
          "expression": "X̃_t = Σ_{k=0}^{l-1} w_k * X_{t-k}, where l = floor(log(τ)/log(1-d))",
          "explanation": "Truncate weights when |w_k| < τ (threshold). Typical τ = 1e-5 gives l ≈ 50-100."
        }
      ]
    },
    {
      "chapter_num": 7,
      "title": "Cross-Validation in Finance",
      "pages": "103-122",
      "topics": ["purged_kfold", "embargo", "cv", "walk_forward"],
      "summary": "Standard k-fold CV leaks information because train and test sets share overlapping label information. Purged k-fold removes training samples that overlap with test labels. Embargo adds a gap between train and test to prevent information leakage from labels near the boundary.",
      "key_points": [
        "CRITICAL: Standard k-fold causes catastrophic overfitting in financial ML - your 62% backtest is likely ~50% real",
        "Purging: Remove from training any sample whose label overlaps with test period",
        "Embargo: After purging, also remove samples within T_embargo of test period boundary",
        "Typical embargo = 1% of total samples, or based on your label's average span",
        "Combinatorial purged cross-validation (CPCV) tests multiple train/test combinations",
        "ALWAYS use purged CV - standard CV will make your backtest look 10-30% better than reality"
      ],
      "formulas": [
        {
          "name": "Purging Condition",
          "expression": "Purge sample i if t_{i,start} < t_{test,end} AND t_{i,end} > t_{test,start}",
          "explanation": "Sample i overlaps with test period if its label's time span intersects the test period."
        },
        {
          "name": "Embargo Period",
          "expression": "T_embargo = pct_embargo * (max(t) - min(t))",
          "explanation": "Typical pct_embargo = 0.01. Remove samples within T_embargo after each test fold boundary."
        }
      ]
    },
    {
      "chapter_num": 8,
      "title": "Feature Importance",
      "pages": "123-142",
      "topics": ["mdi", "mda", "sfi", "feature_importance", "clustering"],
      "summary": "Mean Decrease Impurity (MDI) from tree models is biased toward high-cardinality features. Mean Decrease Accuracy (MDA) with purged CV is more reliable. Single Feature Importance (SFI) tests each feature in isolation. Cluster features before importance analysis to handle multicollinearity.",
      "key_points": [
        "MDI (default sklearn feature_importances_) is biased - do NOT trust it for finance",
        "MDA: Permute feature values, measure accuracy drop with purged CV - most reliable",
        "SFI: Train model on each feature alone - catches non-linear importance MDI might miss",
        "Cluster correlated features BEFORE importance analysis - otherwise importance is split",
        "Features with high MDI but low MDA are likely overfit - red flag",
        "Use substitution effects: If removing feature X has no impact but X is important, another feature carries same info"
      ],
      "formulas": [
        {
          "name": "Mean Decrease Accuracy",
          "expression": "MDA_j = (1/K) * Σ_k [score(D_k^test) - score(D_k^test with j permuted)]",
          "explanation": "Average accuracy drop when feature j is permuted, across K purged CV folds."
        },
        {
          "name": "Single Feature Importance",
          "expression": "SFI_j = CV_score(model trained only on feature j)",
          "explanation": "Cross-validated score using only feature j. Reveals non-linear predictive power."
        }
      ]
    },
    {
      "chapter_num": 11,
      "title": "The Dangers of Backtesting",
      "pages": "169-186",
      "topics": ["backtest_overfitting", "selection_bias", "multiple_testing", "deflated_sharpe"],
      "summary": "Most backtests are overfit due to selection bias (testing many strategies, reporting winners) and improper cross-validation. The Deflated Sharpe Ratio adjusts for the number of trials. The probability of backtest overfitting (PBO) should be reported for every strategy.",
      "key_points": [
        "CRITICAL: A Sharpe of 2.0 from 100 backtests is statistically worthless - expected by chance",
        "Selection bias: If you ran N backtests, your best result's Sharpe is inflated by sqrt(2*log(N))",
        "Haircut Sharpe Ratio: SR_adj = SR * (1 - 1/(2*N)) for N trials - this is generous",
        "Deflated Sharpe accounts for: non-normality, serial correlation, and number of trials",
        "PBO (Probability of Backtest Overfitting): Use CPCV to estimate - should be < 0.20",
        "YOUR 62% BACKTEST vs 0% LIVE: Classic sign of overfitting - apply these corrections immediately"
      ],
      "formulas": [
        {
          "name": "Deflated Sharpe Ratio",
          "expression": "DSR = (SR - SR_benchmark) / σ(SR) * (1 - γ₃/6 * SR + γ₄/24 * SR²)",
          "explanation": "Adjusts for skewness (γ₃) and kurtosis (γ₄). SR_benchmark = sqrt(2)*erfinv(1-1/N) for N trials."
        },
        {
          "name": "Expected Max Sharpe from N Trials",
          "expression": "E[max(SR)] ≈ (1-γ)*Z^(-1)(1-1/N) + γ*Z^(-1)(1-1/(N*e))",
          "explanation": "Where γ ≈ 0.5772 (Euler constant). For N=100 trials, expected max SR ≈ 2.3 even if true SR=0."
        }
      ]
    },
    {
      "chapter_num": 12,
      "title": "Backtesting through Cross-Validation",
      "pages": "187-200",
      "topics": ["walk_forward", "cpcv", "combinatorial_cv", "out_of_sample"],
      "summary": "Walk-forward analysis is a single path through data - it can be lucky or unlucky. Combinatorial purged cross-validation (CPCV) tests all possible train/test combinations, giving a distribution of performance metrics and probability of overfitting.",
      "key_points": [
        "Walk-forward gives ONE Sharpe estimate - insufficient for statistical inference",
        "CPCV partitions data into N groups, uses N-2 for training, 2 for testing, all combinations",
        "CPCV produces N*(N-1)/2 Sharpe estimates - can compute confidence intervals",
        "PBO from CPCV: If more than 20% of combinations underperform benchmark, likely overfit",
        "Stochastic deflated Sharpe: Use distribution from CPCV to compute probability SR > threshold",
        "IMPLEMENT THIS: Your system needs CPCV, not simple train/test split"
      ],
      "formulas": [
        {
          "name": "CPCV Number of Paths",
          "expression": "N_paths = C(N, 2) = N*(N-1)/2",
          "explanation": "For N=10 groups, you get 45 independent backtest paths for statistical analysis."
        },
        {
          "name": "Probability of Backtest Overfitting",
          "expression": "PBO = (# paths with SR < 0) / N_paths",
          "explanation": "Fraction of CPCV paths that would have lost money. PBO > 0.20 indicates overfitting."
        }
      ]
    }
  ]
}
